{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVACjKD9b1kKmInUIvkj99",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mavitu56/SLMs/blob/main/SLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Workflow Completo: Aprimoramento e Comparação de SLMs\n",
        "\n",
        "Este notebook é a versão final, completa e funcional do plano para aprimorar e comparar um Small Language Model (SLM) usando Destilação de Conhecimento e Engenharia de Prompt."
      ],
      "metadata": {
        "id": "markdown_intro_main"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Instalação e Importações Globais"
      ],
      "metadata": {
        "id": "markdown_deps"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hg4t9Rbq06Wq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5826daae-c630-4ffd-d041-bc373a06fbcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers \"datasets==2.19.0\" evaluate peft accelerate ipywidgets bitsandbytes sentencepiece pandas matplotlib --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import re\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from collections import Counter\n",
        "from tqdm.notebook import tqdm\n",
        "from datasets import load_dataset, Dataset\n",
        "from evaluate import load\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from torch.optim import AdamW\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "imports-central"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Funções da Fase 0: Configuração do Ambiente"
      ],
      "metadata": {
        "id": "markdown_phase_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def configure_task_and_metrics():\n",
        "    \"\"\"Carrega o dataset e as métricas para a tarefa de avaliação.\"\"\"\n",
        "    print(\"--- Step 0.1: Configuring Task (SQuAD) and Metrics ---\")\n",
        "    validation_dataset = load_dataset(\"squad\", split=\"validation\")\n",
        "    squad_metric = load(\"squad\")\n",
        "    print(\"Dataset e métricas carregados.\")\n",
        "    return validation_dataset, squad_metric\n",
        "\n",
        "def load_model(model_id):\n",
        "    \"\"\"Carrega um modelo e seu tokenizador a partir de um ID.\"\"\"\n",
        "    print(f\"\\n--- Loading Model: {model_id} ---\")\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        # Adiciona um pad token se não existir, comum para modelos de geração\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id, device_map=\"auto\", torch_dtype=torch.bfloat16\n",
        "        )\n",
        "        print(\"Modelo carregado com sucesso.\")\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao carregar o modelo: {e}\")\n",
        "        return None, None\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "def load_model_quantized(model_id):\n",
        "    \"\"\"\n",
        "    Carrega um modelo e seu tokenizador com quantização de 4-bit para economizar memória.\n",
        "    \"\"\"\n",
        "    print(f\"\\\\n--- Loading Quantized Model: {model_id} ---\")\n",
        "\n",
        "    # Configuração para carregar o modelo em 4-bit\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        if tokenizer.pad_token is None:\n",
        "            # Muitos modelos de instrução não têm pad_token, use o eos_token\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            device_map=\"auto\",\n",
        "            quantization_config=quantization_config,\n",
        "            torch_dtype=torch.bfloat16 # Consistente com o compute_dtype\n",
        "        )\n",
        "        print(\"Modelo quantizado carregado com sucesso.\")\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao carregar o modelo: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def prepare_datasets(validation_dataset):\n",
        "    \"\"\"Prepara todos os subconjuntos de dados necessários.\"\"\"\n",
        "    print(\"\\n--- Step 0.4: Preparing Datasets ---\")\n",
        "    squad_train_full = load_dataset(\"squad\", split=\"train\")\n",
        "    # Usamos um subset menor para o KD Transfer para agilizar o treinamento\n",
        "    kd_transfer_set = squad_train_full.shuffle(seed=42).select(range(10000))\n",
        "    icl_cot_set = squad_train_full.select(range(5))\n",
        "    datasets = {\n",
        "        \"evaluation\": validation_dataset,\n",
        "        \"kd_transfer\": kd_transfer_set,\n",
        "        \"icl_cot_examples\": icl_cot_set\n",
        "    }\n",
        "    print(\"Datasets preparados.\")\n",
        "    return datasets"
      ],
      "metadata": {
        "id": "4Hlnxt1s06Wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Funções da Fase 1: Destilação de Conhecimento (KD)"
      ],
      "metadata": {
        "id": "markdown_phase_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KDConfig:\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    TEMPERATURE = 2.0\n",
        "    LEARNING_RATE = 5e-5\n",
        "    NUM_EPOCHS = 3 # Reduzido para uma execução mais rápida de exemplo. Aumente para 3 em um treino real.\n",
        "\n",
        "# Pad logits até o mesmo comprimento (com zeros no final, que terão pouco peso na softmax)\n",
        "def pad_to_len(logits, target_len):\n",
        "    pad_len = target_len - logits.shape[1]\n",
        "    if pad_len > 0:\n",
        "        pad = torch.zeros((logits.shape[0], pad_len, logits.shape[2]), device=logits.device)\n",
        "        logits = torch.cat([logits, pad], dim=1)\n",
        "    return logits\n",
        "\n",
        "# --- Técnica 1.1: KD Base (Logit Matching) ---\n",
        "def compute_distillation_loss(student_logits, teacher_logits, temperature):\n",
        "    \"\"\"Calcula a perda de Destilação de Conhecimento (KL Divergence).\"\"\"\n",
        "    soft_teacher_probs = F.softmax(teacher_logits / temperature, dim=-1)\n",
        "    soft_student_log_probs = F.log_softmax(student_logits / temperature, dim=-1)\n",
        "    distillation_loss = F.kl_div(soft_student_log_probs, soft_teacher_probs, reduction='batchmean')\n",
        "    return (temperature**2) * distillation_loss\n",
        "\n",
        "def run_base_kd_training(student_model, teacher_model, student_tokenizer, teacher_tokenizer, kd_dataset, config, output_dir):\n",
        "    \"\"\"Executa o loop de treinamento para a Destilação de Conhecimento base.\"\"\"\n",
        "    print(f\"\\n--- Running Base Knowledge Distillation -> saving to {output_dir} ---\")\n",
        "    lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\", task_type=TaskType.CAUSAL_LM)\n",
        "    peft_student_model = get_peft_model(student_model, lora_config)\n",
        "    optimizer = AdamW(peft_student_model.parameters(), lr=config.LEARNING_RATE)\n",
        "    teacher_model.eval()\n",
        "\n",
        "    for epoch in range(config.NUM_EPOCHS):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{config.NUM_EPOCHS}\")\n",
        "        peft_student_model.train()\n",
        "        for batch in tqdm(kd_dataset.shuffle(seed=epoch).select(range(200)), desc=f\"Epoch {epoch+1}\"):\n",
        "            prompt = f\"Contexto: {batch['context']}\\n\\nPergunta: {batch['question']}\"\n",
        "\n",
        "            # Tokeniza os inputs\n",
        "            teacher_inputs = teacher_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(config.DEVICE)\n",
        "            student_inputs = student_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(config.DEVICE)\n",
        "\n",
        "            # Gera os logits\n",
        "            with torch.no_grad():\n",
        "                teacher_logits = teacher_model(**teacher_inputs).logits\n",
        "            student_logits = peft_student_model(**student_inputs).logits\n",
        "\n",
        "            # Verifica compatibilidade de vocab\n",
        "            if student_logits.shape[-1] != teacher_logits.shape[-1]:\n",
        "                raise ValueError(\n",
        "                    f\"Os tamanhos dos vocabulários do estudante ({student_logits.shape[-1]}) e do professor ({teacher_logits.shape[-1]}) são diferentes. \"\n",
        "                    \"A destilação de logits direta não é possível. Use a Destilação de Explicações.\"\n",
        "                )\n",
        "\n",
        "            # Pad para comprimento igual\n",
        "            max_len = max(student_logits.shape[1], teacher_logits.shape[1])\n",
        "            student_logits = pad_to_len(student_logits, max_len)\n",
        "            teacher_logits = pad_to_len(teacher_logits, max_len)\n",
        "\n",
        "            # Cálculo da loss e otimização\n",
        "            loss = compute_distillation_loss(student_logits, teacher_logits, config.TEMPERATURE)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "    peft_student_model.save_pretrained(output_dir)\n",
        "    student_tokenizer.save_pretrained(output_dir)\n",
        "    return peft_student_model\n",
        "    print(f\"Modelo destilado salvo em {output_dir}\")\n",
        "\n",
        "\n",
        "# --- Técnica 1.2: Destilação de Explicações ---\n",
        "def generate_explanation_dataset(teacher_model, tokenizer, transfer_dataset):\n",
        "    \"\"\"Usa o modelo professor para gerar um dataset com raciocínios.\"\"\"\n",
        "    print(\"\\n--- Generating Explanation-Augmented Dataset ---\")\n",
        "    prompt_template = \"A partir do contexto, responda à pergunta. Explique seu raciocínio passo a passo e termine com a resposta final. Formato: [RACIOCÍNIO] ... <sep> [RESPOSTA] ...\\n\\nContexto: {context}\\nPergunta: {question}\"\n",
        "    new_data = {\"text\": []}\n",
        "    for example in tqdm(transfer_dataset.select(range(200)), desc=\"Generating Explanations\"):\n",
        "        full_prompt = prompt_template.format(context=example['context'], question=example['question'])\n",
        "        inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(KDConfig.DEVICE)\n",
        "        outputs = teacher_model.generate(**inputs, max_new_tokens=256, pad_token_id=tokenizer.eos_token_id)\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        new_data[\"text\"].append(generated_text)\n",
        "    return Dataset.from_dict(new_data)\n",
        "\n",
        "def run_explanation_kd_training(student_model, tokenizer, explanation_dataset, config, output_dir):\n",
        "    \"\"\"Treina o estudante para gerar as explicações do professor.\"\"\"\n",
        "    print(f\"\\n--- Running Explanation Distillation -> saving to {output_dir} ---\")\n",
        "    lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\", task_type=TaskType.CAUSAL_LM)\n",
        "    peft_student_model = get_peft_model(student_model, lora_config)\n",
        "    training_args = TrainingArguments(output_dir=\"./results/explanation_training\", num_train_epochs=config.NUM_EPOCHS, learning_rate=config.LEARNING_RATE, report_to=\"none\")\n",
        "    trainer = Trainer(model=peft_student_model, args=training_args, train_dataset=explanation_dataset, tokenizer=tokenizer, data_collator=lambda data: {'input_ids': torch.stack([f['input_ids'] for f in data]), 'attention_mask': torch.stack([f['attention_mask'] for f in data]), 'labels': torch.stack([f['input_ids'] for f in data])})\n",
        "    trainer.train()\n",
        "    peft_student_model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "    print(f\"Modelo de explicação destilada salvo em {output_dir}\")"
      ],
      "metadata": {
        "id": "kd_functions"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Funções da Fase 2: Avaliação com Engenharia de Prompt"
      ],
      "metadata": {
        "id": "markdown_phase_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_zero_shot_baseline(model, tokenizer, evaluation_dataset, metrics_calculator):\n",
        "    \"\"\"Executa a avaliação de baseline (Zero-Shot Simples).\"\"\"\n",
        "    prompt = \"Contexto: {context}\\n\\nPergunta: {question}\\n\\nResposta:\"\n",
        "    return run_generic_evaluation(model, tokenizer, evaluation_dataset, metrics_calculator, prompt, \"Zero-Shot Baseline\")\n",
        "\n",
        "def run_few_shot_icl(model, tokenizer, evaluation_dataset, icl_examples, num_shots, metrics_calculator):\n",
        "    \"\"\"Executa a avaliação com In-Context Learning (Few-Shot).\"\"\"\n",
        "    prefix = \"\".join([f\"Contexto: {ex['context']}\\nPergunta: {ex['question']}\\nResposta: {ex['answers']['text'][0]}\\n\\n---\\n\\n\" for ex in icl_examples.select(range(num_shots))])\n",
        "    prompt = prefix + \"Contexto: {context}\\n\\nPergunta: {question}\\n\\nResposta:\"\n",
        "    return run_generic_evaluation(model, tokenizer, evaluation_dataset, metrics_calculator, prompt, f\"{num_shots}-Shot ICL\")\n",
        "\n",
        "def run_zero_shot_cot(model, tokenizer, evaluation_dataset, metrics_calculator):\n",
        "    \"\"\"Executa a avaliação com Zero-Shot Chain-of-Thought.\"\"\"\n",
        "    prompt = \"Contexto: {context}\\n\\nPergunta: {question}\\n\\nPense passo a passo. A resposta final é:\"\n",
        "    return run_generic_evaluation(model, tokenizer, evaluation_dataset, metrics_calculator, prompt, \"Zero-Shot CoT\")\n",
        "\n",
        "def run_self_consistency_cot(model, tokenizer, evaluation_dataset, metrics_calculator, num_paths=5):\n",
        "    \"\"\"Executa a avaliação com Auto-Consistência sobre o Zero-Shot CoT.\"\"\"\n",
        "    print(f\"\\n--- Running Self-Consistency ({num_paths} paths) ---\")\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.eval()\n",
        "    prompt_template = \"Contexto: {context}\\n\\nPergunta: {question}\\n\\nPense passo a passo. A resposta final é:\"\n",
        "\n",
        "    predictions, references = [], []\n",
        "    for example in tqdm(evaluation_dataset.select(range(20)), desc=\"Self-Consistency Eval\"):\n",
        "        prompt = prompt_template.format(context=example['context'], question=example['question'])\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        path_answers = []\n",
        "        for _ in range(num_paths):\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(**inputs, max_new_tokens=150, do_sample=True, temperature=0.7, top_k=50, pad_token_id=tokenizer.eos_token_id)\n",
        "            completion = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(prompt):].strip()\n",
        "            match = re.search(r'A resposta final é:\\s*(.*)', completion, re.IGNORECASE)\n",
        "            if match: path_answers.append(match.group(1).strip())\n",
        "\n",
        "        final_prediction = Counter(path_answers).most_common(1)[0][0] if path_answers else \"\"\n",
        "        predictions.append({'id': example['id'], 'prediction_text': final_prediction})\n",
        "        references.append({'id': example['id'], 'answers': example['answers']})\n",
        "    return metrics_calculator.compute(predictions=predictions, references=references)\n",
        "\n",
        "def run_generic_evaluation(model, tokenizer, eval_dataset, metrics_calculator, prompt_template, strategy_name, eval_subset_size=10000):\n",
        "    \"\"\"Função genérica que executa a avaliação para a maioria das estratégias de prompt.\"\"\"\n",
        "    print(f\"\\n--- Running Evaluation: {strategy_name} ---\")\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.eval()\n",
        "    predictions, references = [], []\n",
        "\n",
        "    for example in tqdm(eval_dataset.select(range(eval_subset_size)), desc=f\"Evaluating {strategy_name}\"):\n",
        "        prompt = prompt_template.format(context=example['context'], question=example['question'])\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1536, truncation=True).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs, max_new_tokens=60, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "        full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        predicted_answer = full_text[len(prompt):].strip()\n",
        "\n",
        "        if \"A resposta final é:\" in prompt:\n",
        "            match = re.search(r'A resposta final é:\\s*(.*)', predicted_answer, re.IGNORECASE)\n",
        "            predicted_answer = match.group(1).strip() if match else predicted_answer.split('\\n')[-1].strip()\n",
        "\n",
        "        predictions.append({'id': example['id'], 'prediction_text': predicted_answer})\n",
        "        references.append({'id': example['id'], 'answers': example['answers']})\n",
        "\n",
        "    return metrics_calculator.compute(predictions=predictions, references=references)"
      ],
      "metadata": {
        "id": "prompting_functions"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Execução do Pipeline (Modo Simulado)\n",
        "\n",
        "A célula abaixo executa o pipeline em **modo de simulação**. Ela não treina nem avalia os modelos de verdade, mas gera um relatório com dados de exemplo para demonstrar a estrutura de análise. Para uma execução real, você deve primeiro executar as células de treinamento da Fase 1 e depois rodar o pipeline com `simulation_mode=False`."
      ],
      "metadata": {
        "id": "markdown_run_pipeline"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: login hugging face\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "Nwo8_KdnLleN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "97a51d4763014252bff5aebe0cbaf2a7",
            "c965c03d3d5f40d5aaf77e09e4084a88",
            "79c66e3472d24425891eb30426541848",
            "638588f0a46e4266be171650b903bff2",
            "89fc6e6e66e74fc5831d485f10a37eb3",
            "90ff111bca824a2e9688771391f54116",
            "3cfd3a18285b4944a944ff9113662780",
            "6aecc54b475945f388e4e477907e3ddb",
            "41c8bec1f9a74e1bbfd1e17e90b6470c",
            "505e65fdde724253b4dd685cf0060f9a",
            "0e21c5f8a09548c6bea942a72ccfaef1",
            "dab9c2a00e0848f093b37ce5e96f57c0",
            "d5f42c7ee1d94c2e9c5872562fc60335",
            "0451c8141ded4eb9870a55213dfe3a8a",
            "d46b72085d034041bbef130b4c4e5f5a",
            "88cb4de61af14da49db3b8f409406337",
            "41cffeed2e06448d99d96bb1f5398deb",
            "00830943c39245fd94f696f2f5f15b0b",
            "fd7b64fbd59b43bcb44afeb13435150b",
            "b7d49b3da8724570ab7045b3c9ea0ad2"
          ]
        },
        "outputId": "10a7f66c-eca2-4165-f9d8-d16fe3b9b302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "97a51d4763014252bff5aebe0cbaf2a7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "# --- FASE 0: SETUP ---\n",
        "print(\"--- FASE 0: CONFIGURANDO AMBIENTE E DADOS ---\")\n",
        "validation_data, metrics_calc = configure_task_and_metrics()\n",
        "prepared_data = prepare_datasets(validation_data)\n",
        "config = KDConfig()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ua3Yzpa1FqiZ",
        "outputId": "bd1e180d-c038-44a6-bde8-d87f046535ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- FASE 0: CONFIGURANDO AMBIENTE E DADOS ---\n",
            "--- Step 0.1: Configuring Task (SQuAD) and Metrics ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset e métricas carregados.\n",
            "\n",
            "--- Step 0.4: Preparing Datasets ---\n",
            "Datasets preparados.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FASE 1: TREINAMENTO (Opcional) ---\n",
        "print(\"\\n--- FASE 1: EXECUTANDO TREINAMENTO DOS MODELOS KD ---\")\n",
        "TEACHER_MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "STUDENT_MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "# Carregue os modelos usando a nova função\n",
        "teacher_model, teacher_tokenizer = load_model_quantized(TEACHER_MODEL_ID)\n",
        "student_model, student_tokenizer = load_model_quantized(STUDENT_MODEL_ID)\n",
        "teacher_model_ext = teacher_model\n",
        "student_model_base = student_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158,
          "referenced_widgets": [
            "ce34fab61a994bc59665656f4b6cdbad",
            "b4752dc8e3894bd5a721f36665603e12",
            "c8896c81c20c4361b176fb0e6e87d7f8",
            "6cfb9973ec7b4c22ac6d4293bd98e690",
            "7490d15a01164461914131cb23ac5cd2",
            "1c6af5c5e30c4de0bf82bd3458cc47eb",
            "733ae9c0f9c54bcf9e79e3ae91edd1bc",
            "f2133fa581f44a268f4615259c695756",
            "ab06a176d0f546ad81388d10090a3d49",
            "4edd2a2b1f9d4d17bf6b7d97ae983255",
            "cd16675dcb9e47b1bdb7f6201cda55f6"
          ]
        },
        "id": "MqUkrUezHOHm",
        "outputId": "7b52acbf-9050-45a8-85ae-33ccc3611542"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- FASE 1: EXECUTANDO TREINAMENTO DOS MODELOS KD ---\n",
            "\\n--- Loading Quantized Model: mistralai/Mistral-7B-Instruct-v0.2 ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce34fab61a994bc59665656f4b6cdbad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo quantizado carregado com sucesso.\n",
            "\\n--- Loading Quantized Model: TinyLlama/TinyLlama-1.1B-Chat-v1.0 ---\n",
            "Modelo quantizado carregado com sucesso.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Treina o modelo de KD Base\n",
        "run_base_kd_training(\n",
        "    student_model=student_model_base, teacher_model=teacher_model_ext, student_tokenizer=student_tokenizer, teacher_tokenizer=teacher_tokenizer,\n",
        "    kd_dataset=prepared_data['kd_transfer'], config=config, output_dir=\"./results/kd_base_model\"\n",
        ")"
      ],
      "metadata": {
        "id": "wdYRU8tIFsj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Treina o modelo de Auto-Destilação (requer um professor \"especialista\")\n",
        "print(\"\\n--- Treinando professor para Auto-Destilação ---\")\n",
        "# A forma mais simples de criar um professor especialista é com um fine-tuning padrão.\n",
        "# Aqui, vamos simular isso treinando o próprio estudante com KD por uma época.\n",
        "specialist_teacher = run_base_kd_training(\n",
        "    student_model=student_model_base, teacher_model=teacher_model_ext, student_tokenizer=student_tokenizer, teacher_tokenizer=teacher_tokenizer,\n",
        "    kd_dataset=prepared_data['kd_transfer'], config=config, output_dir=\"./results/specialist_teacher_for_self_distillation\"\n",
        ")"
      ],
      "metadata": {
        "id": "DZQQ2HVxF37p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ADAPTER_PATH = \"results/specialist_teacher_for_self_distillation\"\n",
        "print(f\"Carregando adaptadores de: {ADAPTER_PATH}...\")\n",
        "# A mágica acontece aqui: PeftModel aplica os adaptadores sobre o modelo base\n",
        "specialist_teacher = PeftModel.from_pretrained(student_model_base, ADAPTER_PATH)\n",
        "print(\"Adaptadores carregados e aplicados.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "femw-YOnJ_LW",
        "outputId": "4b3c0769-6a39-4b82-f179-ceb359829c1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando adaptadores de: results/specialist_teacher_for_self_distillation...\n",
            "Adaptadores carregados e aplicados.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Executando Auto-Destilação ---\")\n",
        "run_base_kd_training(\n",
        "    student_model=student_model_base, teacher_model=specialist_teacher, student_tokenizer=student_tokenizer, teacher_tokenizer=teacher_tokenizer,\n",
        "    kd_dataset=prepared_data['kd_transfer'], config=config, output_dir=\"./results/kd_self_distilled_model\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "30d211df5e044d6a8945d6c0492cebc6",
            "8022b6f8e98c4fe3b8507c21819d1316",
            "4e3feae131414d168a0ac7247166f579",
            "2a9d6cd06de54cddbd38f469417eea58",
            "cd0841782d784b19a5a40f2efb850277",
            "d2904f0699924601ae424b3e93f8a5c6",
            "c1dc5577a6114514869c539fde7f2210",
            "e39253c81537490f9537dd5aef95b75a",
            "66f224729eb9410085c2c7ca44f7d9a9",
            "c064388e54504f7abf3bcef3cd43f31b",
            "44ad05a616cd494e8766ab76a690073a",
            "a05e7c7afd654bbdb9be92c640d0f654",
            "2192d5f5abd7429ea2ef76a0afbc91a4",
            "573e32875bd84b04971b834ae3689d67",
            "6bc08df77623428cb408712ab55d2162",
            "fc6418383b114aeb9e50c7d14b93640f",
            "3cd5c5605c2542e2aafdc8d9a33644cf",
            "9df9e1ef363d49f88f902129852228ac",
            "e30f5f82cfc5404cb80253ec9768cb09",
            "27ea6afdeae94f76a1557523734575a0",
            "538bb830629141cc9d1e313af654de3e",
            "0b6610d3a282424792cc570b71cce5f4",
            "72eccee00ebb4d84b2809e999e7ad286",
            "061a0c4960724fe08d5ec9b5f5b4c7b0",
            "8590a9689fb441a08cd0fda4966bf95b",
            "23c020f1955149b9902476e0ad652300",
            "0fe2c66408b945dea075a741cf2c6324",
            "50702f7897af4edaa67f596aec968b0d",
            "60865d22112a429cada49ffce2d9399d",
            "06389ce79da84827b2acad9cb148390d",
            "48e94a6675e64950890b997b858c075b",
            "2cf3ee0dc123432d88140ed9230f281d",
            "2702101666d54f198a86f002d74f4d1e"
          ]
        },
        "id": "JNWLhmWAF6Pt",
        "outputId": "8d6c9dec-b102-45a4-a0a8-498f92ad7ae3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Executando Auto-Destilação ---\n",
            "\n",
            "--- Running Base Knowledge Distillation -> saving to ./results/kd_self_distilled_model ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 1:   0%|          | 0/200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30d211df5e044d6a8945d6c0492cebc6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 2:   0%|          | 0/200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a05e7c7afd654bbdb9be92c640d0f654"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 3:   0%|          | 0/200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "72eccee00ebb4d84b2809e999e7ad286"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LlamaForCausalLM(\n",
              "      (model): LlamaModel(\n",
              "        (embed_tokens): Embedding(32000, 2048)\n",
              "        (layers): ModuleList(\n",
              "          (0-21): 22 x LlamaDecoderLayer(\n",
              "            (self_attn): LlamaAttention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=256, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
              "              (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
              "              (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "        (rotary_emb): LlamaRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Treina o modelo de Destilação de Explicações\n",
        "explanation_dataset = generate_explanation_dataset(teacher_model_ext, teacher_tokenizer, prepared_data['kd_transfer'])\n",
        "run_explanation_kd_training(\n",
        "    student_model=student_model_base, tokenizer=student_tokenizer, explanation_dataset=explanation_dataset,\n",
        "    config=config, output_dir=\"./results/explanation_distilled_model\"\n",
        ")\n",
        "print(\"\\n--- FASE DE TREINAMENTO CONCLUÍDA ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305,
          "referenced_widgets": [
            "0359d5ef33764583b3d490065e1b8742",
            "af3a269b88984cc880120d88733b0791",
            "6feb51d6e0dd4a9da7831e0cf0cba656",
            "45f15da1bf9a4199b0f63538cb98c958",
            "b01538ea3f734218880ac8961fcdc6dd",
            "fb1a1cfa039e4dcbb8acc8980df9e04d",
            "8873a4a148f446ebac1a9f7fe01f8abd",
            "a1bd2b29343d42aa9b18fd9ecd89ffe8",
            "c09d16b7271c40cd97842dee18c52155",
            "5713a03b68914020ba7d59a71f1ebfb9",
            "e503952068b846b5912c739a405cc65c"
          ]
        },
        "id": "UPZ8vYOcF_f4",
        "outputId": "bb059a59-c771-4b78-9c49-8215bbe50aad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Generating Explanation-Augmented Dataset ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating Explanations:   0%|          | 0/200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0359d5ef33764583b3d490065e1b8742"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "run_explanation_kd_training() got an unexpected keyword argument 'student_tokenizer'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-3499024544>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 3. Treina o modelo de Destilação de Explicações\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mexplanation_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_explanation_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteacher_model_ext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepared_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'kd_transfer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m run_explanation_kd_training(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mstudent_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstudent_model_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudent_tokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstudent_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_tokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mteacher_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplanation_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplanation_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./results/explanation_distilled_model\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: run_explanation_kd_training() got an unexpected keyword argument 'student_tokenizer'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FASE 2 & 3: AVALIAÇÃO E ANÁLISE ---\n",
        "print(\"\\n--- FASES 2 & 3: EXECUTANDO AVALIAÇÃO COMPLETA E GERANDO RELATÓRIO ---\")\n",
        "model_definitions = {\n",
        "    \"SLM Base\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    \"KD Base\": \"./results/kd_base_model\",\n",
        "    \"KD Auto-Destilado\": \"./results/kd_self_distilled_model\",\n",
        "    \"KD com Explicações\": \"./results/explanation_distilled_model\"\n",
        "}\n",
        "prompting_strategies = {\n",
        "    \"Zero-Shot Simples\": run_zero_shot_baseline,\n",
        "    \"ICL (k=3)\": lambda m, t, d, met: run_few_shot_icl(m, t, d, prepared_data['icl_cot_examples'], 3, met),\n",
        "    \"Zero-Shot CoT\": run_zero_shot_cot,\n",
        "    \"Auto-Consistência (n=5)\": lambda m, t, d, met: run_self_consistency_cot(m, t, d, met, 5)\n",
        "}\n",
        "results_df = pd.DataFrame(index=model_definitions.keys(), columns=prompting_strategies.keys())\n",
        "\n",
        "# Carrega o modelo base uma vez para aplicar os adaptadores PEFT\n",
        "base_model_for_eval, tokenizer = load_model(STUDENT_MODEL_ID)\n",
        "\n",
        "for alias, path in model_definitions.items():\n",
        "    print(f\"\\n>>>> AVALIANDO MODELO: {alias} <<<<\")\n",
        "    if alias == \"SLM Base\":\n",
        "        model = base_model_for_eval\n",
        "    else:\n",
        "        try:\n",
        "            # Carrega o modelo base com o adaptador LoRA treinado\n",
        "            model = PeftModel.from_pretrained(base_model_for_eval, path)\n",
        "            model = model.merge_and_unload() # Opcional: mescla os pesos para acelerar a inferência\n",
        "        except Exception as e:\n",
        "            print(f\"Não foi possível carregar o modelo treinado de '{path}'. Pulando. Erro: {e}\")\n",
        "            continue\n",
        "\n",
        "    for strat_name, strat_func in prompting_strategies.items():\n",
        "        # A função de avaliação genérica agora é chamada por suas wrappers específicas\n",
        "        results = strat_func(model, tokenizer, prepared_data['evaluation'], metrics_calc)\n",
        "        results_df.loc[alias, strat_name] = f\"{results['f1']:.1f} / {results['exact_match']:.1f}\"\n",
        "\n",
        "# --- RELATÓRIO FINAL ---\n",
        "print(\"\\n\\n## Relatório Final de Análise ##\")\n",
        "numeric_df = results_df.applymap(lambda x: float(x.split('/')[0]) if isinstance(x, str) else 0)\n",
        "\n",
        "print(\"\\n--- Matriz de Resultados (F1-Score / Exact Match) ---\")\n",
        "print(results_df.to_markdown())\n",
        "\n",
        "print(\"\\n--- Conclusões da Análise ---\")\n",
        "avg_kd_performance = numeric_df.drop(\"SLM Base\").mean(axis=1)\n",
        "print(f\"- Melhor Técnica de KD (Média F1): '{avg_kd_performance.idxmax()}' ({avg_kd_performance.max():.2f})\")\n",
        "avg_prompt_performance = numeric_df.mean(axis=0)\n",
        "print(f\"- Melhor Estratégia de Prompting (Média F1): '{avg_prompt_performance.idxmax()}' ({avg_prompt_performance.max():.2f})\")\n",
        "best_combo = numeric_df.stack().idxmax()\n",
        "print(f\"- Melhor Sinergia (Modelo + Prompt): '{best_combo[0]}' + '{best_combo[1]}' com F1 de {numeric_df.max().max():.2f}\")\n",
        "\n",
        "print(\"\\nGerando gráfico de comparação...\")\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "numeric_df.plot(kind='bar', ax=ax, title='Comparação de Performance (F1-Score) entre Modelos e Estratégias')\n",
        "ax.set_ylabel(\"F1-Score\")\n",
        "ax.set_xlabel(\"Versão do Modelo\")\n",
        "ax.tick_params(axis='x', rotation=20)\n",
        "plt.legend(title='Estratégia de Prompt', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"real_run_full_performance_comparison.png\")\n",
        "print(\"Gráfico salvo em 'real_run_full_performance_comparison.png'.\")"
      ],
      "metadata": {
        "id": "QSiS-aXpGD8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_real_workflow(True, 10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6c44beb786b74def8f14df70ddb7eaed",
            "68d5c0d514744470aaab51cf1de1fc42",
            "572d06d741144794a4b3fe7d53f92dab",
            "88c1f5ff7f0b4e76ae8d7b79262307d8",
            "245532895d3c469bb95e2a3220daee39",
            "1ae8ac03b0144d4bb1788a9562c85eb1",
            "aa49035b8a2d4fc7b22e851ea0922eb7",
            "cfedac69aea8476a8cc5614ffe47247c",
            "a7a7f465073e47d1b79dcbe046c394d5",
            "2d852277010a49cfbf15f25b7baffb19",
            "8c27fa2e408347f7a455f94fe9a972b2",
            "4b01e7b73b2a4e2b8f2cf68d73aaece5",
            "d7d2919909cf48a18d36215e2256f8ef",
            "662e9212501f4daeaf471e99b07804e2",
            "8da6687059c34aaf9f34faea6fd7ed7f",
            "e909c8477eab48439fa0ca2aa1f4d029",
            "e78942556bbc47c8833d2bac676caaff",
            "0a23c3308198438293dea2eb277cda45",
            "4a91caf605db4137a3838dae93ca7fa6",
            "c34824e565c24464952cec2f872c9e0d",
            "944ddb347ff24917b19be51fc608171f",
            "43c9589dc43f46d78273570b5ca5d090",
            "5c20688970b64a318c535fef1a393a01",
            "0d187c96ab084b2686c7ef1c478d25f3",
            "97526f3b0f1647868cc148886f1e0ec5",
            "a109dc32a68f498295fc4dd05dadf928",
            "5e69082303c04245986c986ed85f1ead",
            "1103855be57242269c043183d80bfbf1",
            "c4aa23330bb44954999ffd1807e172be",
            "dea69cf21a52446d8ebc0df2718f5c9b",
            "f6007e1c6a164b1480004bc4986f9de5",
            "a88600a49cf14a9cba65c30675f7f9a8",
            "d3a25990979c401c94382b552eef3585",
            "b389ad5873ca44c782f0006ff1d43f2d",
            "61f7a4a31eda428ca8da983fed10a088",
            "c6a06c073bd944df867343649e2c7db8",
            "84553fa6a4f043a4b084773b9f8469d0",
            "77d3de2b17b440cdaf254a85e797a3f6",
            "39fb476f8a65404b9f5eb59433f3002f",
            "e603f53f56fb4011bdda7992ae7f7095",
            "07dc1e2b71694b9386c9c177e1f5edcf",
            "cb311728dd184a05ac828304e4ee6a72",
            "588b66c1d863483e84cea50ab2ca2198",
            "09a90cf5d13f45f590dfcd609fdd3a70",
            "ddec43e6a95e40e384651063c41f83b0",
            "a55b7ced7ef643888232c81f3dc0ccba",
            "659fc9db2da84ad589d68412791b9d4a",
            "bc0d6e39faf845c78741b93b05e1d1a7",
            "31a1bc6675cd41ab923aedb3353c91fe",
            "f2dc9511fdf24d9fb808141f5cc93ace",
            "5a5e9eb125ad4d6f93ed125b54044ab3",
            "cf1d27b914d243feb5625fc81d65585a",
            "2ad1f0f0e996477ab175090992a95097",
            "131ddf72d4d74a1ea815feb8d0210348",
            "d9bbdbba0314462e949b2616caea177b",
            "ea0f7548d8fc4524bf130d2c82631f13",
            "d65363cdd6114671a3e2aae1fe6f80f5",
            "5210f0d4d9d4492d8410a2c2ac4a613e",
            "21f246702ab843e29d19daa3440a28e5",
            "b7f9d5dc1b28456f93e76874f8006156",
            "1eff408977704926b4efec7b1b0e7f5f",
            "3e851ee8d7c440d4815aedeca4a31bae",
            "8708461c826e44c0b58d545c8a221cc3",
            "13bb69f9be904c34a431e4ce7e4b4d22",
            "cb9f9ab772434bc7a7dd38166deb100a",
            "afff5010316245bfa278e7132d34d7e9",
            "c7b67488b370409b8468f76c9eff0a57",
            "e694b9ef27c44ac6a13d129529d8d1e6",
            "7ec8e9ad225b443dac080bcb29bd3354",
            "65d8305dfc2949a3a7b3f86138b6075d",
            "cf2985d02e9e4b84a0db865048478aa1",
            "b02bb52a2856475aaf2b791f5f67c963",
            "6214a718074643e6bf69261ae8ca6c8e",
            "3ef854fe7479413ab1877442663de394",
            "7acc7e0805414de0a205dbf09058e848",
            "06c1db6e74cb4fac88c06032707d9410",
            "80ae2a475da1459dbc11c8fd9fa8e8d5"
          ]
        },
        "id": "e2zr_LJsNAIM",
        "outputId": "3ecf0cfe-97c8-4631-db16-34ccd6031aba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- FASE 0: CONFIGURANDO AMBIENTE E DADOS ---\n",
            "--- Step 0.1: Configuring Task (SQuAD) and Metrics ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset e métricas carregados.\n",
            "\n",
            "--- Step 0.4: Preparing Datasets ---\n",
            "Datasets preparados.\n",
            "\n",
            "--- FASE 1: EXECUTANDO TREINAMENTO DOS MODELOS KD ---\n",
            "\\n--- Loading Quantized Model: mistralai/Mistral-7B-Instruct-v0.2 ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c44beb786b74def8f14df70ddb7eaed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo quantizado carregado com sucesso.\n",
            "\\n--- Loading Quantized Model: TinyLlama/TinyLlama-1.1B-Chat-v1.0 ---\n",
            "Modelo quantizado carregado com sucesso.\n",
            "\n",
            "--- Running Base Knowledge Distillation -> saving to ./results/kd_base_model ---\n",
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 1:   0%|          | 0/200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4b01e7b73b2a4e2b8f2cf68d73aaece5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 2:   0%|          | 0/200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c20688970b64a318c535fef1a393a01"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 3:   0%|          | 0/200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b389ad5873ca44c782f0006ff1d43f2d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo destilado salvo em ./results/kd_base_model\n",
            "\n",
            "--- Treinando professor para Auto-Destilação ---\n",
            "\n",
            "--- Running Base Knowledge Distillation -> saving to ./results/specialist_teacher_for_self_distillation ---\n",
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 1:   0%|          | 0/200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ddec43e6a95e40e384651063c41f83b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 2:   0%|          | 0/200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea0f7548d8fc4524bf130d2c82631f13"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 3:   0%|          | 0/200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7b67488b370409b8468f76c9eff0a57"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo destilado salvo em ./results/specialist_teacher_for_self_distillation\n",
            "\n",
            "--- Executando Auto-Destilação ---\n",
            "\n",
            "--- Running Base Knowledge Distillation -> saving to ./results/kd_self_distilled_model ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'eval'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-1249565240>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_real_workflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-3583675260>\u001b[0m in \u001b[0;36mrun_real_workflow\u001b[0;34m(run_training, eval_subset_size)\u001b[0m\n\u001b[1;32m     44\u001b[0m         )\n\u001b[1;32m     45\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Executando Auto-Destilação ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         run_base_kd_training(\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mstudent_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstudent_model_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspecialist_teacher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudent_tokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstudent_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_tokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mteacher_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mkd_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'kd_transfer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./results/kd_self_distilled_model\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-3051188584>\u001b[0m in \u001b[0;36mrun_base_kd_training\u001b[0;34m(student_model, teacher_model, student_tokenizer, teacher_tokenizer, kd_dataset, config, output_dir)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mpeft_student_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_peft_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlora_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeft_student_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mteacher_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'eval'"
          ]
        }
      ]
    }
  ]
}