{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfyXz6AJWY79+lZ43Ws8Qo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mavitu56/SLMs/blob/main/SLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ly1YF3BDk4S-"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers datasets evaluate peft accelerate ipywidgets sentencepiece pandas matplotlib --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando as bibliotecas necessárias da Hugging Face, como especificado no plano\n",
        "from datasets import load_dataset\n",
        "from evaluate import load\n",
        "\n",
        "def configure_task_and_metrics():\n",
        "    \"\"\"\n",
        "    Define e carrega o dataset e as métricas para a tarefa de avaliação.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Definição da Tarefa e Dataset\n",
        "    # Tarefa: Question Answering (QA) Extrativo\n",
        "    # Dataset: SQuAD (Stanford Question Answering Dataset)\n",
        "    # Motivo: Benchmark padrão e disponível no Hugging Face Datasets\n",
        "    try:\n",
        "        print(\"Carregando o dataset SQuAD...\")\n",
        "        # Usaremos apenas o split de validação para a avaliação final, como um conjunto de teste fixo\n",
        "        squad_dataset_validation = load_dataset(\"squad\", split=\"validation\")\n",
        "        print(\"Dataset SQuAD (validação) carregado com sucesso.\")\n",
        "        print(f\"Número de exemplos no conjunto de validação: {len(squad_dataset_validation)}\")\n",
        "        print(f\"Exemplo de dado: {squad_dataset_validation[0]}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao carregar o dataset SQuAD: {e}\")\n",
        "        squad_dataset_validation = None\n",
        "\n",
        "    # 2. Definição das Métricas\n",
        "    # Métricas: F1-score e Exact Match, padrões para SQuAD.\n",
        "    # Carregadas usando a biblioteca 'evaluate' da Hugging Face\n",
        "    try:\n",
        "        print(\"\\nCarregando as métricas para SQuAD...\")\n",
        "        squad_metric = load(\"squad\")\n",
        "        print(\"Métricas para SQuAD carregadas com sucesso.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao carregar a métrica SQuAD: {e}\")\n",
        "        squad_metric = None\n",
        "\n",
        "    return squad_dataset_validation, squad_metric"
      ],
      "metadata": {
        "id": "FqRzAfMQlEo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_student_model():\n",
        "    \"\"\"\n",
        "    Seleciona e carrega o SLM base (estudante) e seu tokenizador.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Ponto 2: Selecionando SLM Base (Estudante) ---\")\n",
        "\n",
        "    # Modelo escolhido: Gemma-2b da Google\n",
        "    model_id = \"google/gemma-2b\"\n",
        "    print(f\"Modelo base selecionado: {model_id}\")\n",
        "\n",
        "    try:\n",
        "        # Carregando o tokenizador\n",
        "        print(f\"Carregando o tokenizador para {model_id}...\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        print(\"Tokenizador carregado com sucesso.\")\n",
        "\n",
        "        # Carregando o modelo\n",
        "        # A utilização de torch.bfloat16 é para otimizar o uso de memória em GPUs compatíveis\n",
        "        print(f\"Carregando o modelo {model_id}...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            device_map=\"auto\", # Tenta usar a GPU se disponível\n",
        "            torch_dtype=torch.bfloat16\n",
        "        )\n",
        "        print(\"Modelo base (estudante) carregado com sucesso.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao carregar o modelo ou tokenizador: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "9juKSTRwsF6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FILE: model_loader.py\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "def select_teacher_model():\n",
        "    \"\"\"\n",
        "    Selects and loads the teacher LLM and its tokenizer.\n",
        "    The teacher model is a larger, more powerful model used for Knowledge Distillation.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Step 3: Selecting The Teacher Model ---\")\n",
        "\n",
        "    # Model selected: Llama-3-8B-Instruct from Meta\n",
        "    # Reason: A powerful open-source model to act as the 'teacher'.\n",
        "    model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "    print(f\"Teacher model selected: {model_id}\")\n",
        "\n",
        "    try:\n",
        "        # Loading the tokenizer\n",
        "        print(f\"Loading tokenizer for {model_id}...\")\n",
        "        # Note: For Llama-3, you might need to request access or use a gated Hugging Face token\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        print(\"Tokenizer loaded successfully.\")\n",
        "\n",
        "        # Loading the model\n",
        "        print(f\"Loading model {model_id}...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            device_map=\"auto\", # Tries to use GPU if available\n",
        "            torch_dtype=torch.bfloat16\n",
        "        )\n",
        "        print(\"Teacher model loaded successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading the teacher model or tokenizer: {e}\")\n",
        "        print(\"Please ensure you have access to the model on Hugging Face Hub.\")\n",
        "        return None, None\n",
        "\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "t7018VrwtmWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FILE: data_preparation.py\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "def prepare_datasets(validation_dataset):\n",
        "    \"\"\"\n",
        "    Prepares the datasets needed for training and advanced prompting techniques.\n",
        "\n",
        "    This includes:\n",
        "    1. The KD Transfer Set: A subset of the training data for knowledge distillation.\n",
        "    2. The ICL/CoT Set: A few high-quality examples for in-context learning.\n",
        "    3. The Evaluation Set: The pre-loaded validation dataset.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Step 4: Preparing Datasets ---\")\n",
        "\n",
        "    try:\n",
        "        print(\"Loading SQuAD training split...\")\n",
        "        squad_train_full = load_dataset(\"squad\", split=\"train\")\n",
        "        print(\"SQuAD training split loaded successfully.\")\n",
        "\n",
        "        # For resource-limited environments, we'll create a smaller subset for KD.\n",
        "        # Shuffling ensures the subset is random and representative.\n",
        "        kd_transfer_set_size = 10000\n",
        "        print(f\"Creating a random subset of {kd_transfer_set_size} examples for the KD Transfer Set...\")\n",
        "        kd_transfer_set = squad_train_full.shuffle(seed=42).select(range(kd_transfer_set_size))\n",
        "\n",
        "        # Select a few high-quality examples for In-Context Learning (ICL) and Chain-of-Thought (CoT)\n",
        "        icl_cot_set_size = 5\n",
        "        print(f\"Selecting {icl_cot_set_size} examples for the ICL/CoT Set...\")\n",
        "        icl_cot_set = squad_train_full.select(range(icl_cot_set_size)) # Taking the first few for simplicity\n",
        "\n",
        "        datasets = {\n",
        "            \"evaluation\": validation_dataset,\n",
        "            \"kd_transfer\": kd_transfer_set,\n",
        "            \"icl_cot_examples\": icl_cot_set\n",
        "        }\n",
        "\n",
        "        print(\"\\nDatasets prepared successfully:\")\n",
        "        print(f\" - Evaluation Set size: {len(datasets['evaluation'])}\")\n",
        "        print(f\" - KD Transfer Set size: {len(datasets['kd_transfer'])}\")\n",
        "        print(f\" - ICL/CoT Examples size: {len(datasets['icl_cot_examples'])}\")\n",
        "\n",
        "        return datasets\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during dataset preparation: {e}\")\n",
        "        return None\n",
        "\n",
        "# How to use this function in a notebook:\n",
        "#\n",
        "# from task_setup import configure_task_and_metrics\n",
        "# validation_set, metrics = configure_task_and_metrics()\n",
        "#\n",
        "# if validation_set:\n",
        "#     prepared_datasets = prepare_datasets(validation_set)"
      ],
      "metadata": {
        "id": "tHv1-2VkuVJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class KDConfig:\n",
        "    # Device setup\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Model IDs (already decided)\n",
        "    STUDENT_MODEL_ID = \"google/gemma-2b\"\n",
        "    TEACHER_MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "    # Distillation hyperparameters from the paper\n",
        "    TEMPERATURE = 2.0  # The 'T' that softens the probabilities\n",
        "    ALPHA = 0.5        # The weight between KD loss and a potential standard loss\n",
        "\n",
        "    # Training hyperparameters\n",
        "    LEARNING_RATE = 5e-5\n",
        "    NUM_EPOCHS = 3\n",
        "    BATCH_SIZE = 4     # Adjust based on your GPU memory"
      ],
      "metadata": {
        "id": "uon02ovGyyr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def compute_distillation_loss(student_logits, teacher_logits, temperature, alpha):\n",
        "    \"\"\"\n",
        "    Computes the Knowledge Distillation loss.\n",
        "    This loss encourages the student's output distribution to match the teacher's.\n",
        "\n",
        "    Implements the formula: L_total = alpha * T^2 * L_KD\n",
        "    Note: The original paper's loss is used here. We omit L_CE for pure distillation.\n",
        "    The T^2 factor scales the gradient, it's part of the original KD formulation.\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate the distillation loss (L_KD) based on softened logits\n",
        "    soft_teacher_probs = F.softmax(teacher_logits / temperature, dim=-1)\n",
        "    soft_student_log_probs = F.log_softmax(student_logits / temperature, dim=-1)\n",
        "\n",
        "    # KL Divergence Loss\n",
        "    distillation_loss = F.kl_div(\n",
        "        soft_student_log_probs,\n",
        "        soft_teacher_probs,\n",
        "        reduction='batchmean' # Averages the loss over the batch\n",
        "    )\n",
        "\n",
        "    # The final loss is scaled by T^2 as proposed by Hinton et al.\n",
        "    # We focus purely on the distillation loss (alpha=1.0) for this base implementation\n",
        "    # L_total = alpha * (temperature**2) * distillation_loss + (1-alpha) * L_CE\n",
        "    # For now, let's assume L_CE is not used, so alpha = 1.0\n",
        "\n",
        "    final_loss = (temperature**2) * distillation_loss\n",
        "\n",
        "    return final_loss"
      ],
      "metadata": {
        "id": "-EGAkBToy19R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm # For progress bars\n",
        "\n",
        "def run_base_kd_training(student_model, teacher_model, tokenizer, kd_dataset, config):\n",
        "    \"\"\"\n",
        "    Runs the base Knowledge Distillation training loop.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Phase 1, Step 1: Base Knowledge Distillation (Logit Matching) ---\")\n",
        "\n",
        "    # 1. PEFT Setup: Prepare student model for efficient fine-tuning\n",
        "    lora_config = LoraConfig(\n",
        "        r=16, # Rank of the update matrices\n",
        "        lora_alpha=32,\n",
        "        target_modules=[\"q_proj\", \"v_proj\"], # Target specific layers\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=TaskType.CAUSAL_LM\n",
        "    )\n",
        "    peft_student_model = get_peft_model(student_model, lora_config)\n",
        "    peft_student_model.print_trainable_parameters()\n",
        "\n",
        "    # 2. Optimizer: Targets only the trainable PEFT parameters\n",
        "    optimizer = AdamW(peft_student_model.parameters(), lr=config.LEARNING_RATE)\n",
        "\n",
        "    # 3. Freeze the teacher model's weights\n",
        "    teacher_model.eval()\n",
        "\n",
        "    # 4. Training Loop\n",
        "    peft_student_model.to(config.DEVICE)\n",
        "    teacher_model.to(config.DEVICE)\n",
        "\n",
        "    for epoch in range(config.NUM_EPOCHS):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{config.NUM_EPOCHS}\")\n",
        "        peft_student_model.train() # Set student to training mode\n",
        "        total_loss = 0\n",
        "\n",
        "        for step, batch in enumerate(tqdm(kd_dataset)): # Using a dataloader is recommended\n",
        "            # Note: A proper dataloader would handle tokenization and batching\n",
        "            # This is a simplified representation of the logic\n",
        "\n",
        "            # Dummy tokenized inputs - replace with actual tokenization\n",
        "            inputs = tokenizer(\"dummy text\", return_tensors=\"pt\").to(config.DEVICE)\n",
        "\n",
        "            # Get teacher logits (inference mode, no gradients)\n",
        "            with torch.no_grad():\n",
        "                teacher_outputs = teacher_model(**inputs)\n",
        "                teacher_logits = teacher_outputs.logits\n",
        "\n",
        "            # Get student logits (training mode)\n",
        "            student_outputs = peft_student_model(**inputs)\n",
        "            student_logits = student_outputs.logits\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = compute_distillation_loss(\n",
        "                student_logits,\n",
        "                teacher_logits,\n",
        "                temperature=config.TEMPERATURE,\n",
        "                alpha=config.ALPHA\n",
        "            )\n",
        "\n",
        "            # Backpropagation to update student's PEFT weights\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    print(\"Base KD training complete.\")\n",
        "\n",
        "    # 5. Save the resulting model\n",
        "    # This saves only the trained adapter weights, which is very efficient\n",
        "    output_dir = \"./results/kd_base_model\"\n",
        "    peft_student_model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "    print(f\"Distilled model (KD Base) saved to {output_dir}\")\n",
        "\n",
        "    return peft_student_model"
      ],
      "metadata": {
        "id": "SdfofFj9y-SP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FILE: create_teacher.py\n",
        "# (Este é um exemplo conceitual de um script de fine-tuning padrão)\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "def create_specialist_teacher(base_model, tokenizer, train_dataset):\n",
        "    \"\"\"\n",
        "    Fine-tunes a base model on a specific task to create a 'specialist' teacher.\n",
        "    \"\"\"\n",
        "    print(\"--- Self-Distillation: Creating the Specialist Teacher ---\")\n",
        "\n",
        "    # Define os argumentos de treinamento\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results/specialist_teacher\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=4,\n",
        "        learning_rate=2e-5,\n",
        "        logging_dir='./logs',\n",
        "        report_to=\"none\" # Desativa integrações como wandb\n",
        "    )\n",
        "\n",
        "    # Cria a instância do Trainer\n",
        "    trainer = Trainer(\n",
        "        model=base_model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset, # O dataset de treino do SQuAD\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    # Inicia o treinamento\n",
        "    print(\"Starting fine-tuning to create the teacher model...\")\n",
        "    trainer.train()\n",
        "    print(\"Specialist teacher model created successfully.\")\n",
        "\n",
        "    # Salva o modelo professor\n",
        "    teacher_output_dir = \"./models/gemma_2b_squad_teacher\"\n",
        "    trainer.save_model(teacher_output_dir)\n",
        "    tokenizer.save_pretrained(teacher_output_dir)\n",
        "\n",
        "    return base_model"
      ],
      "metadata": {
        "id": "XVoUDQug1a2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FILE: run_self_distillation.py\n",
        "# (Reutiliza a função de KD Base, apenas muda os modelos de entrada)\n",
        "\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "# Supondo que as funções anteriores já foram definidas:\n",
        "# from kd_training import run_base_kd_training\n",
        "# from kd_config import KDConfig\n",
        "\n",
        "# 1. Carregue o professor \"especialista\" que você treinou na Fase A\n",
        "teacher_model_path = \"./models/gemma_2b_squad_teacher\"\n",
        "specialist_teacher_model = AutoModelForCausalLM.from_pretrained(teacher_model_path)\n",
        "\n",
        "# 2. Carregue um modelo estudante \"novo\" (não treinado)\n",
        "student_model, tokenizer = select_student_model() # Função que já definimos\n",
        "\n",
        "# 3. Execute o mesmo loop de treinamento de KD Base\n",
        "# O código é o mesmo, a diferença é o 'teacher_model' que estamos usando\n",
        "print(\"\\n--- Running Self-Distillation training ---\")\n",
        "run_base_kd_training(\n",
        "    student_model=student_model,\n",
        "    teacher_model=specialist_teacher_model, # A única grande mudança está aqui!\n",
        "    tokenizer=tokenizer,\n",
        "    kd_dataset=prepared_datasets['kd_transfer'],\n",
        "    config=KDConfig()\n",
        ")"
      ],
      "metadata": {
        "id": "6xlD9w2_1iQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FILE: generate_explanations.py\n",
        "\n",
        "from tqdm import tqdm\n",
        "from datasets import Dataset\n",
        "\n",
        "def generate_explanation_dataset(teacher_model, teacher_tokenizer, transfer_dataset, device):\n",
        "    \"\"\"\n",
        "    Uses the teacher model to generate step-by-step explanations and answers.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Explanation Distillation: Generating new dataset ---\")\n",
        "\n",
        "    # Prompt que instrui o modelo a gerar o raciocínio e a resposta\n",
        "    prompt_template = \"\"\"A partir do contexto abaixo, responda à pergunta.\n",
        "    Explique seu raciocínio passo a passo e termine com a resposta final.\n",
        "    Use o formato: [RACIOCÍNIO] sua explicação aqui <sep> [RESPOSTA] sua resposta aqui.\n",
        "\n",
        "    Contexto: {context}\n",
        "    Pergunta: {question}\n",
        "    \"\"\"\n",
        "\n",
        "    new_data = {\"input_text\": [], \"teacher_output\": []}\n",
        "\n",
        "    teacher_model.to(device)\n",
        "    teacher_model.eval()\n",
        "\n",
        "    for example in tqdm(transfer_dataset):\n",
        "        # Cria o prompt completo\n",
        "        full_prompt = prompt_template.format(context=example['context'], question=example['question'])\n",
        "        new_data[\"input_text\"].append(full_prompt)\n",
        "\n",
        "        # Gera a saída do professor\n",
        "        inputs = teacher_tokenizer(full_prompt, return_tensors=\"pt\").to(device)\n",
        "        outputs = teacher_model.generate(**inputs, max_new_tokens=256, pad_token_id=teacher_tokenizer.eos_token_id)\n",
        "        generated_text = teacher_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Extrai apenas a parte gerada pelo modelo\n",
        "        teacher_completion = generated_text[len(full_prompt):]\n",
        "        new_data[\"teacher_output\"].append(teacher_completion)\n",
        "\n",
        "    # Converte o dicionário em um novo Dataset da Hugging Face\n",
        "    explanation_dataset = Dataset.from_dict(new_data)\n",
        "    print(\"Explanation-augmented dataset created successfully.\")\n",
        "\n",
        "    return explanation_dataset"
      ],
      "metadata": {
        "id": "mMzGMKn91j0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FILE: run_explanation_distillation.py\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "def run_explanation_kd_training(student_model, tokenizer, explanation_dataset):\n",
        "    \"\"\"\n",
        "    Fine-tunes the student model on the teacher-generated explanations.\n",
        "    This is a standard causal language modeling task.\n",
        "    \"\"\"\n",
        "    print(\"--- Explanation Distillation: Training the student ---\")\n",
        "\n",
        "    # O modelo é treinado para gerar a sequência [RACIOCÍNIO] <sep> [RESPOSTA]\n",
        "\n",
        "    # Função para tokenizar os dados: entrada e saída são concatenadas\n",
        "    def preprocess_function(examples):\n",
        "        inputs = examples['input_text']\n",
        "        targets = examples['teacher_output']\n",
        "        # Concatenamos para que o modelo aprenda a gerar o alvo a partir da entrada\n",
        "        model_inputs = [inp + out + tokenizer.eos_token for inp, out in zip(inputs, targets)]\n",
        "        return tokenizer(model_inputs, truncation=True, max_length=512)\n",
        "\n",
        "    tokenized_dataset = explanation_dataset.map(preprocess_function, batched=True, remove_columns=explanation_dataset.column_names)\n",
        "\n",
        "    # Argumentos de treinamento\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results/explanation_distilled_model\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=2, # Menor batch size pois os textos são mais longos\n",
        "        learning_rate=2e-5,\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "    # Trainer para a tarefa de modelagem de linguagem causal\n",
        "    trainer = Trainer(\n",
        "        model=student_model, # Usamos o PEFT-wrapped model para eficiência\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    print(\"Starting training on explanation data...\")\n",
        "    trainer.train()\n",
        "    print(\"Explanation distillation training complete.\")\n",
        "\n",
        "    # Salva o modelo final\n",
        "    output_dir = \"./models/gemma_2b_explanation_distilled\"\n",
        "    trainer.save_model(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    return student_model"
      ],
      "metadata": {
        "id": "1FB2B1Fj1lzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FILE: run_evaluation.py\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "def run_zero_shot_baseline(model, tokenizer, evaluation_dataset, metrics_calculator):\n",
        "    \"\"\"\n",
        "    Executa a avaliação de baseline (Zero-Shot Simples) em um determinado modelo.\n",
        "    Isto estabelece o desempenho base de cada modelo com uma instrução direta.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Iniciando Avaliação Baseline (Zero-Shot) para o modelo: {model.config._name_or_path} ---\")\n",
        "\n",
        "    # Move o modelo para a GPU, se disponível\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Listas para armazenar as previsões e as respostas de referência\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    # Loop sobre o conjunto de avaliação\n",
        "    for example in tqdm(evaluation_dataset):\n",
        "        context = example['context']\n",
        "        question = example['question']\n",
        "        example_id = example['id']\n",
        "\n",
        "        # Formato do prompt Zero-Shot\n",
        "        # Uma instrução direta e simples, conforme o plano\n",
        "        prompt = f\"\"\"Use o contexto abaixo para responder à pergunta.\n",
        "\n",
        "Contexto: {context}\n",
        "\n",
        "Pergunta: {question}\n",
        "\n",
        "Resposta:\"\"\"\n",
        "\n",
        "        # Tokeniza o prompt e gera a resposta\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "        # Decodifica a resposta e limpa o texto\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        # Extrai apenas a resposta (o texto após o prompt original)\n",
        "        predicted_answer = generated_text[len(prompt):].strip()\n",
        "\n",
        "        # Adiciona a previsão no formato esperado pela métrica SQuAD\n",
        "        predictions.append({'id': example_id, 'prediction_text': predicted_answer})\n",
        "        # Adiciona a referência no formato esperado pela métrica SQuAD\n",
        "        references.append({'id': example_id, 'answers': example['answers']})\n",
        "\n",
        "    # Calcula as métricas finais\n",
        "    print(\"Calculando as métricas finais...\")\n",
        "    results = metrics_calculator.compute(predictions=predictions, references=references)\n",
        "\n",
        "    print(f\"\\nResultados do Baseline para {model.config._name_or_path}:\")\n",
        "    print(f\"  - Exact Match: {results['exact_match']:.2f}%\")\n",
        "    print(f\"  - F1-score: {results['f1']:.2f}%\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Exemplo de como você chamaria esta função em seu notebook:\n",
        "#\n",
        "# # Supondo que 'student_model', 'tokenizer', 'prepared_datasets' e 'squad_metric' já foram carregados\n",
        "# baseline_results = run_zero_shot_baseline(\n",
        "#     model=student_model, # Passe o modelo que quer avaliar\n",
        "#     tokenizer=tokenizer,\n",
        "#     evaluation_dataset=prepared_datasets['evaluation'],\n",
        "#     metrics_calculator=squad_metric\n",
        "# )"
      ],
      "metadata": {
        "id": "sQiLMib24PLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FILE: run_evaluation.py (continuação)\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "def run_few_shot_icl(model, tokenizer, evaluation_dataset, icl_examples, num_shots, metrics_calculator):\n",
        "    \"\"\"\n",
        "    Executa a avaliação com In-Context Learning (Few-Shot).\n",
        "    Testa o desempenho do modelo ao fornecer 'k' exemplos no prompt.\n",
        "    \"\"\"\n",
        "    k = num_shots\n",
        "    print(f\"\\n--- Iniciando Avaliação ICL ({k}-Shot) para o modelo: {model.config._name_or_path} ---\")\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # 1. Construir o prefixo do prompt com os exemplos de ICL\n",
        "    few_shot_prefix = \"\"\n",
        "    for i in range(k):\n",
        "        example = icl_examples[i]\n",
        "        # A resposta correta no SQuAD é uma lista, pegamos a primeira\n",
        "        answer = example['answers']['text'][0]\n",
        "        few_shot_prefix += f\"Contexto: {example['context']}\\n\\nPergunta: {example['question']}\\n\\nResposta: {answer}\\n\\n---\\n\\n\"\n",
        "\n",
        "    # Listas para previsões e referências\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    # 2. Loop sobre o conjunto de avaliação\n",
        "    for example in tqdm(evaluation_dataset):\n",
        "        context = example['context']\n",
        "        question = example['question']\n",
        "        example_id = example['id']\n",
        "\n",
        "        # Formato do prompt Few-Shot: prefixo com exemplos + nova pergunta\n",
        "        prompt = few_shot_prefix + f\"Contexto: {context}\\n\\nPergunta: {question}\\n\\nResposta:\"\n",
        "\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        # Extrai apenas a resposta (o texto após o prompt original)\n",
        "        predicted_answer = generated_text[len(prompt):].strip()\n",
        "\n",
        "        predictions.append({'id': example_id, 'prediction_text': predicted_answer})\n",
        "        references.append({'id': example_id, 'answers': example['answers']})\n",
        "\n",
        "    # 3. Calcula as métricas\n",
        "    print(f\"Calculando as métricas finais para {k}-shot...\")\n",
        "    results = metrics_calculator.compute(predictions=predictions, references=references)\n",
        "\n",
        "    print(f\"\\nResultados do ICL ({k}-Shot) para {model.config._name_or_path}:\")\n",
        "    print(f\"  - Exact Match: {results['exact_match']:.2f}%\")\n",
        "    print(f\"  - F1-score: {results['f1']:.2f}%\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Exemplo de como você chamaria esta função em seu notebook para testar k=1, 3 e 5\n",
        "#\n",
        "# # Supondo que tudo já foi carregado\n",
        "# for k_shots in [1, 3, 5]:\n",
        "#     icl_results = run_few_shot_icl(\n",
        "#         model=student_model,\n",
        "#         tokenizer=tokenizer,\n",
        "#         evaluation_dataset=prepared_datasets['evaluation'],\n",
        "#         icl_examples=prepared_datasets['icl_cot_examples'], # Nosso conjunto de exemplos de alta qualidade\n",
        "#         num_shots=k_shots,\n",
        "#         metrics_calculator=squad_metric\n",
        "#     )"
      ],
      "metadata": {
        "id": "15IVCH0X7Vo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FILE: run_evaluation.py (continuação)\n",
        "import re\n",
        "\n",
        "def run_zero_shot_cot(model, tokenizer, evaluation_dataset, metrics_calculator):\n",
        "    \"\"\"\n",
        "    Executa a avaliação com Zero-Shot Chain-of-Thought.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Iniciando Avaliação Zero-Shot CoT para: {model.config._name_or_path} ---\")\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    for example in tqdm(evaluation_dataset):\n",
        "        context = example['context']\n",
        "        question = example['question']\n",
        "        example_id = example['id']\n",
        "\n",
        "        # Prompt Zero-Shot CoT: Adiciona a instrução para pensar\n",
        "        prompt = f\"\"\"Contexto: {context}\n",
        "\n",
        "Pergunta: {question}\n",
        "\n",
        "Pense passo a passo. Responda no formato \"A resposta final é: [sua resposta]\".\n",
        "\"\"\"\n",
        "        # A instrução de formato ajuda no pós-processamento.\n",
        "\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs, max_new_tokens=150, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        completion = generated_text[len(prompt):].strip()\n",
        "\n",
        "        # Pós-processamento: Extrair a resposta final\n",
        "        # Procuramos pelo padrão \"A resposta final é:\" ou pegamos a última linha\n",
        "        match = re.search(r'A resposta final é:\\s*(.*)', completion, re.IGNORECASE)\n",
        "        if match:\n",
        "            predicted_answer = match.group(1).strip()\n",
        "        else:\n",
        "            # Se o padrão não for encontrado, usamos uma heurística (ex: última linha)\n",
        "            predicted_answer = completion.split('\\n')[-1].strip()\n",
        "\n",
        "        predictions.append({'id': example_id, 'prediction_text': predicted_answer})\n",
        "        references.append({'id': example_id, 'answers': example['answers']})\n",
        "\n",
        "    print(\"Calculando as métricas finais para Zero-Shot CoT...\")\n",
        "    results = metrics_calculator.compute(predictions=predictions, references=references)\n",
        "\n",
        "    print(f\"\\nResultados do Zero-Shot CoT para {model.config._name_or_path}:\")\n",
        "    print(f\"  - Exact Match: {results['exact_match']:.2f}%\")\n",
        "    print(f\"  - F1-score: {results['f1']:.2f}%\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "CRlScYT-76KJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FILE: run_evaluation.py (continuação)\n",
        "import torch\n",
        "import re\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "def run_self_consistency_cot(model, tokenizer, evaluation_dataset, metrics_calculator, num_paths=5):\n",
        "    \"\"\"\n",
        "    Executa a avaliação com Auto-Consistência sobre o Zero-Shot CoT.\n",
        "    Gera múltiplos caminhos de raciocínio e escolhe a resposta mais comum.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Iniciando Avaliação com Auto-Consistência ({num_paths} caminhos) para: {model.config._name_or_path} ---\")\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    for example in tqdm(evaluation_dataset):\n",
        "        context = example['context']\n",
        "        question = example['question']\n",
        "        example_id = example['id']\n",
        "\n",
        "        # Prompt é o mesmo do Zero-Shot CoT\n",
        "        prompt = f\"\"\"Contexto: {context}\n",
        "\n",
        "Pergunta: {question}\n",
        "\n",
        "Pense passo a passo. Responda no formato \"A resposta final é: [sua resposta]\".\n",
        "\"\"\"\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        # 1. Gerar múltiplos caminhos de resposta\n",
        "        path_answers = []\n",
        "        for _ in range(num_paths):\n",
        "            with torch.no_grad():\n",
        "                # Ativamos a amostragem para gerar saídas diversas\n",
        "                outputs = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=150,\n",
        "                    do_sample=True, # Essencial para a diversidade\n",
        "                    temperature=0.7, # Controla a aleatoriedade\n",
        "                    top_k=50,\n",
        "                    pad_token_id=tokenizer.eos_token_id\n",
        "                )\n",
        "\n",
        "            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            completion = generated_text[len(prompt):].strip()\n",
        "\n",
        "            # Pós-processamento para extrair a resposta de cada caminho\n",
        "            match = re.search(r'A resposta final é:\\s*(.*)', completion, re.IGNORECASE)\n",
        "            if match:\n",
        "                path_answers.append(match.group(1).strip())\n",
        "\n",
        "        # 2. Votação majoritária para encontrar a resposta mais consistente\n",
        "        if path_answers:\n",
        "            # Usa collections.Counter para contar a frequência de cada resposta\n",
        "            most_common_answer = Counter(path_answers).most_common(1)[0][0]\n",
        "            final_prediction = most_common_answer\n",
        "        else:\n",
        "            final_prediction = \"\" # Se nenhum caminho produzir uma resposta válida\n",
        "\n",
        "        predictions.append({'id': example_id, 'prediction_text': final_prediction})\n",
        "        references.append({'id': example_id, 'answers': example['answers']})\n",
        "\n",
        "    print(\"Calculando as métricas finais para Auto-Consistência...\")\n",
        "    results = metrics_calculator.compute(predictions=predictions, references=references)\n",
        "\n",
        "    print(f\"\\nResultados da Auto-Consistência para {model.config._name_or_path}:\")\n",
        "    print(f\"  - Exact Match: {results['exact_match']:.2f}%\")\n",
        "    print(f\"  - F1-score: {results['f1']:.2f}%\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Exemplo de como chamar a função em seu notebook:\n",
        "#\n",
        "# self_consistency_results = run_self_consistency_cot(\n",
        "#     model=student_model,\n",
        "#     tokenizer=tokenizer,\n",
        "#     evaluation_dataset=prepared_datasets['evaluation'],\n",
        "#     metrics_calculator=squad_metric,\n",
        "#     num_paths=5 # Define quantos caminhos de raciocínio gerar\n",
        "# )"
      ],
      "metadata": {
        "id": "4WlvflVV8c1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "Fase 3: Avaliação e Comparação Científica Sistemática\n",
        "Este script orquestra a execução de todos os experimentos, coleta as métricas,\n",
        "analisa os resultados e gera um relatório final.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "# --- Simulação das Funções e Modelos das Fases Anteriores ---\n",
        "# Em um cenário real, você importaria estas funções e carregaria os modelos.\n",
        "# Para este exemplo, vamos simular suas saídas para focar na lógica da Fase 3.\n",
        "\n",
        "def load_model_and_tokenizer(model_name):\n",
        "    \"\"\"Função de simulação para carregar um modelo.\"\"\"\n",
        "    print(f\"Carregando modelo simulado: {model_name}...\")\n",
        "    # Retorna um objeto 'model' e 'tokenizer' simulados\n",
        "    return {\"name\": model_name}, {\"name\": f\"{model_name}-tokenizer\"}\n",
        "\n",
        "def run_zero_shot_baseline(model, tokenizer, dataset, metrics):\n",
        "    \"\"\"Simula a execução do baseline.\"\"\"\n",
        "    # Retorna resultados simulados para cada modelo\n",
        "    scores = {\"SLM Base\": 55.3, \"KD Base\": 62.1, \"KD Auto-Destilado\": 59.8, \"KD com Explicações\": 65.5}\n",
        "    score = scores.get(model['name'], 50.0) + (hash(model['name']) % 10) # Adiciona variação\n",
        "    return {'f1': score, 'exact_match': score - 10}\n",
        "\n",
        "def run_few_shot_icl(model, tokenizer, dataset, icl_examples, num_shots, metrics):\n",
        "    \"\"\"Simula a execução do ICL.\"\"\"\n",
        "    base_score = run_zero_shot_baseline(model, tokenizer, dataset, metrics)['f1']\n",
        "    return {'f1': base_score + 5 + num_shots, 'exact_match': base_score - 5 + num_shots}\n",
        "\n",
        "def run_zero_shot_cot(model, tokenizer, dataset, metrics):\n",
        "    \"\"\"Simula a execução do CoT.\"\"\"\n",
        "    base_score = run_zero_shot_baseline(model, tokenizer, dataset, metrics)['f1']\n",
        "    return {'f1': base_score + 8, 'exact_match': base_score - 2}\n",
        "\n",
        "def run_self_consistency_cot(model, tokenizer, dataset, metrics, num_paths):\n",
        "    \"\"\"Simula a execução da Auto-Consistência.\"\"\"\n",
        "    base_score = run_zero_shot_cot(model, tokenizer, dataset, metrics)['f1']\n",
        "    return {'f1': base_score + 4, 'exact_match': base_score + 2}\n",
        "\n",
        "# --- Implementação da Fase 3 ---\n",
        "\n",
        "def step_1_create_experiment_matrix():\n",
        "    \"\"\"\n",
        "    Passo 1: Cria a Matriz de Experimentos.\n",
        "    Define os modelos a serem testados e as estratégias de prompting a serem aplicadas.\n",
        "    \"\"\"\n",
        "    print(\"## Passo 1: Definindo a Matriz de Experimentos ##\\n\")\n",
        "\n",
        "    # Linhas da matriz: Nossas versões de modelo\n",
        "    model_versions = {\n",
        "        \"SLM Base\": \"google/gemma-2b\",\n",
        "        \"KD Base\": \"./models/kd_base_model\",\n",
        "        \"KD Auto-Destilado\": \"./models/kd_self_distilled_model\",\n",
        "        \"KD com Explicações\": \"./models/explanation_distilled_model\"\n",
        "    }\n",
        "\n",
        "    # Colunas da matriz: Nossas estratégias de prompting\n",
        "    prompting_strategies = {\n",
        "        \"Zero-Shot Simples\": run_zero_shot_baseline,\n",
        "        \"ICL (k=3)\": lambda m, t, d, met: run_few_shot_icl(m, t, d, None, 3, met),\n",
        "        \"Zero-Shot CoT\": run_zero_shot_cot,\n",
        "        \"Auto-Consistência (n=5)\": lambda m, t, d, met: run_self_consistency_cot(m, t, d, met, 5)\n",
        "    }\n",
        "\n",
        "    # Cria um DataFrame pandas vazio para armazenar os resultados\n",
        "    results_df = pd.DataFrame(index=model_versions.keys(), columns=prompting_strategies.keys())\n",
        "\n",
        "    return model_versions, prompting_strategies, results_df\n",
        "\n",
        "\n",
        "def step_2_and_3_run_experiments_and_collect_metrics(model_versions, prompting_strategies, results_df):\n",
        "    \"\"\"\n",
        "    Passos 2 & 3: Executa todos os experimentos e coleta as métricas.\n",
        "    Preenche a matriz com os resultados de F1-score / Exact Match.\n",
        "    \"\"\"\n",
        "    print(\"## Passos 2 & 3: Executando Experimentos e Coletando Métricas ##\\n\")\n",
        "\n",
        "    # Simulação de datasets e métricas\n",
        "    dummy_dataset = {}\n",
        "    dummy_metrics = {}\n",
        "\n",
        "    for model_alias, model_path in model_versions.items():\n",
        "        model, tokenizer = load_model_and_tokenizer(model_alias) # Nome do modelo usado como alias\n",
        "\n",
        "        for strategy_name, strategy_func in prompting_strategies.items():\n",
        "            print(f\"Executando '{strategy_name}' para o modelo '{model_alias}'...\")\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Executa a função de avaliação correspondente\n",
        "            results = strategy_func(model, tokenizer, dummy_dataset, dummy_metrics)\n",
        "\n",
        "            # Coleta métricas de desempenho e eficiência (opcional)\n",
        "            f1 = results['f1']\n",
        "            em = results['exact_match']\n",
        "            latency = time.time() - start_time\n",
        "\n",
        "            # Preenche a matriz\n",
        "            results_df.loc[model_alias, strategy_name] = f\"{f1:.1f} / {em:.1f}\"\n",
        "            # Poderíamos adicionar a latência a outra matriz se quiséssemos\n",
        "            print(f\"Resultado (F1/EM): {f1:.1f}/{em:.1f} | Latência: {latency:.2f}s\")\n",
        "\n",
        "    return results_df\n",
        "\n",
        "\n",
        "def step_4_analyze_results(results_df):\n",
        "    \"\"\"\n",
        "    Passo 4: Analisa os resultados para responder às perguntas científicas.\n",
        "    \"\"\"\n",
        "    print(\"\\n## Passo 4: Análise dos Resultados ##\\n\")\n",
        "\n",
        "    # Converte os resultados para um tipo numérico para análise (usando F1 como principal)\n",
        "    numeric_df = results_df.applymap(lambda x: float(x.split('/')[0]))\n",
        "\n",
        "    analysis_summary = []\n",
        "\n",
        "    # Pergunta: Qual técnica de KD foi melhor?\n",
        "    avg_kd_performance = numeric_df.drop(\"SLM Base\").mean(axis=1)\n",
        "    best_kd_model = avg_kd_performance.idxmax()\n",
        "    analysis_summary.append(f\"Melhor Técnica de KD: '{best_kd_model}' com uma média de F1 de {avg_kd_performance.max():.2f}.\")\n",
        "\n",
        "    # Pergunta: Qual estratégia de prompting foi melhor?\n",
        "    avg_prompt_performance = numeric_df.mean(axis=0)\n",
        "    best_prompt_strategy = avg_prompt_performance.idxmax()\n",
        "    analysis_summary.append(f\"Melhor Estratégia de Prompting: '{best_prompt_strategy}' com uma média de F1 de {avg_prompt_performance.max():.2f}.\")\n",
        "\n",
        "    # Pergunta: Houve sinergia?\n",
        "    best_combo = numeric_df.stack().idxmax()\n",
        "    analysis_summary.append(f\"Melhor Sinergia (Combinação): O modelo '{best_combo[0]}' com a estratégia '{best_combo[1]}' atingiu o maior F1 de {numeric_df.max().max():.2f}.\")\n",
        "\n",
        "    print(\"\\nResumo da Análise:\")\n",
        "    for point in analysis_summary:\n",
        "        print(f\"- {point}\")\n",
        "\n",
        "    return analysis_summary, numeric_df\n",
        "\n",
        "\n",
        "def step_5_document_and_report(results_df, numeric_df, analysis_summary):\n",
        "    \"\"\"\n",
        "    Passo 5: Documenta e relata os resultados.\n",
        "    Cria uma visualização e um resumo em texto.\n",
        "    \"\"\"\n",
        "    print(\"\\n## Passo 5: Documentação e Relatório Final ##\\n\")\n",
        "\n",
        "    report = f\"\"\"\n",
        "# Relatório de Aprimoramento e Comparação de SLMs\n",
        "\n",
        "## 1. Resumo da Análise\n",
        "\"\"\"\n",
        "    for point in analysis_summary:\n",
        "        report += f\"- {point}\\n\"\n",
        "\n",
        "    report += \"\\n## 2. Matriz de Resultados (F1-Score / Exact Match)\\n\"\n",
        "    report += results_df.to_markdown()\n",
        "\n",
        "    print(\"Gerando relatório em formato Markdown...\")\n",
        "    print(\"-------------------------------------------\")\n",
        "    print(report)\n",
        "    print(\"-------------------------------------------\")\n",
        "\n",
        "    # Gera um gráfico de barras para a performance do baseline\n",
        "    fig, ax = plt.subplots()\n",
        "    numeric_df['Zero-Shot Simples'].plot(kind='bar', ax=ax, title='Performance Baseline (F1-Score)', legend=False)\n",
        "    ax.set_ylabel(\"F1-Score\")\n",
        "    ax.set_xlabel(\"Versão do Modelo\")\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Salva o gráfico\n",
        "    output_filename = \"baseline_performance_comparison.png\"\n",
        "    plt.savefig(output_filename)\n",
        "    print(f\"\\nGráfico de comparação salvo em '{output_filename}'.\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Orquestra a execução de toda a Fase 3\n",
        "    model_defs, strategy_defs, df = step_1_create_experiment_matrix()\n",
        "    completed_df = step_2_and_3_run_experiments_and_collect_metrics(model_defs, strategy_defs, df)\n",
        "    summary, numeric_results_df = step_4_analyze_results(completed_df)\n",
        "    step_5_document_and_report(completed_df, numeric_results_df, summary)"
      ],
      "metadata": {
        "id": "H80kp_lJ_hRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Workflow Completo: Aprimoramento e Comparação de SLMs\n",
        "\n",
        "Este notebook é a versão final, completa e funcional do plano para aprimorar e comparar um Small Language Model (SLM) usando Destilação de Conhecimento e Engenharia de Prompt."
      ],
      "metadata": {
        "id": "markdown_intro_main"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Instalação e Importações Globais"
      ],
      "metadata": {
        "id": "markdown_deps"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hg4t9Rbq06Wq"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers \"datasets==2.19.0\" evaluate peft accelerate ipywidgets sentencepiece pandas matplotlib --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import re\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from collections import Counter\n",
        "from tqdm.notebook import tqdm\n",
        "from datasets import load_dataset, Dataset\n",
        "from evaluate import load\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from torch.optim import AdamW\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "imports-central"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Funções da Fase 0: Configuração do Ambiente"
      ],
      "metadata": {
        "id": "markdown_phase_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def configure_task_and_metrics():\n",
        "    \"\"\"Carrega o dataset e as métricas para a tarefa de avaliação.\"\"\"\n",
        "    print(\"--- Step 0.1: Configuring Task (SQuAD) and Metrics ---\")\n",
        "    validation_dataset = load_dataset(\"squad\", split=\"validation\")\n",
        "    squad_metric = load(\"squad\")\n",
        "    print(\"Dataset e métricas carregados.\")\n",
        "    return validation_dataset, squad_metric\n",
        "\n",
        "def load_model(model_id):\n",
        "    \"\"\"Carrega um modelo e seu tokenizador a partir de um ID.\"\"\"\n",
        "    print(f\"\\n--- Loading Model: {model_id} ---\")\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        # Adiciona um pad token se não existir, comum para modelos de geração\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id, device_map=\"auto\", torch_dtype=torch.bfloat16\n",
        "        )\n",
        "        print(\"Modelo carregado com sucesso.\")\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao carregar o modelo: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def prepare_datasets(validation_dataset):\n",
        "    \"\"\"Prepara todos os subconjuntos de dados necessários.\"\"\"\n",
        "    print(\"\\n--- Step 0.4: Preparing Datasets ---\")\n",
        "    squad_train_full = load_dataset(\"squad\", split=\"train\")\n",
        "    # Usamos um subset menor para o KD Transfer para agilizar o treinamento\n",
        "    kd_transfer_set = squad_train_full.shuffle(seed=42).select(range(10000))\n",
        "    icl_cot_set = squad_train_full.select(range(5))\n",
        "    datasets = {\n",
        "        \"evaluation\": validation_dataset,\n",
        "        \"kd_transfer\": kd_transfer_set,\n",
        "        \"icl_cot_examples\": icl_cot_set\n",
        "    }\n",
        "    print(\"Datasets preparados.\")\n",
        "    return datasets"
      ],
      "metadata": {
        "id": "4Hlnxt1s06Wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Funções da Fase 1: Destilação de Conhecimento (KD)"
      ],
      "metadata": {
        "id": "markdown_phase_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KDConfig:\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    TEMPERATURE = 2.0\n",
        "    LEARNING_RATE = 5e-5\n",
        "    NUM_EPOCHS = 1 # Reduzido para uma execução mais rápida de exemplo. Aumente para 3 em um treino real.\n",
        "\n",
        "# --- Técnica 1.1: KD Base (Logit Matching) ---\n",
        "def compute_distillation_loss(student_logits, teacher_logits, temperature):\n",
        "    \"\"\"Calcula a perda de Destilação de Conhecimento (KL Divergence).\"\"\"\n",
        "    soft_teacher_probs = F.softmax(teacher_logits / temperature, dim=-1)\n",
        "    soft_student_log_probs = F.log_softmax(student_logits / temperature, dim=-1)\n",
        "    distillation_loss = F.kl_div(soft_student_log_probs, soft_teacher_probs, reduction='batchmean')\n",
        "    return (temperature**2) * distillation_loss\n",
        "\n",
        "def run_base_kd_training(student_model, teacher_model, student_tokenizer, teacher_tokenizer, kd_dataset, config, output_dir):\n",
        "    \"\"\"Executa o loop de treinamento para a Destilação de Conhecimento base.\"\"\"\n",
        "    print(f\"\\n--- Running Base Knowledge Distillation -> saving to {output_dir} ---\")\n",
        "    lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\", task_type=TaskType.CAUSAL_LM)\n",
        "    peft_student_model = get_peft_model(student_model, lora_config)\n",
        "    optimizer = AdamW(peft_student_model.parameters(), lr=config.LEARNING_RATE)\n",
        "    teacher_model.eval()\n",
        "\n",
        "    for epoch in range(config.NUM_EPOCHS):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{config.NUM_EPOCHS}\")\n",
        "        peft_student_model.train()\n",
        "        for batch in tqdm(kd_dataset.shuffle(seed=epoch).select(range(200)), desc=f\"Epoch {epoch+1}\"):\n",
        "            prompt = f\"Contexto: {batch['context']}\\n\\nPergunta: {batch['question']}\"\n",
        "\n",
        "            # Tokeniza para o PROFESSOR com o tokenizador do PROFESSOR\n",
        "            teacher_inputs = teacher_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(config.DEVICE)\n",
        "\n",
        "            # Tokeniza para o ESTUDANTE com o tokenizador do ESTUDANTE\n",
        "            student_inputs = student_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(config.DEVICE)\n",
        "\n",
        "            # Gera os logits de cada modelo com seus respectivos inputs\n",
        "            with torch.no_grad():\n",
        "                teacher_logits = teacher_model(**teacher_inputs).logits\n",
        "\n",
        "            student_logits = peft_student_model(**student_inputs).logits\n",
        "\n",
        "            # A perda é calculada entre os logits, que devem ter o mesmo tamanho de vocabulário\n",
        "            # Se os vocabulários forem diferentes, a destilação de logits não é diretamente aplicável\n",
        "            # e a \"Destilação de Explicações\" se torna a abordagem correta.\n",
        "            # Adicionando uma verificação para evitar o erro:\n",
        "            if student_logits.shape[-1] != teacher_logits.shape[-1]:\n",
        "                raise ValueError(\n",
        "                    f\"Os tamanhos dos vocabulários do estudante ({student_logits.shape[-1]}) e do professor ({teacher_logits.shape[-1]}) são diferentes. \"\n",
        "                    \"A destilação de logits direta não é possível. Use a Destilação de Explicações.\"\n",
        "                )\n",
        "\n",
        "            loss = compute_distillation_loss(student_logits, teacher_logits, config.TEMPERATURE)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "    peft_student_model.save_pretrained(output_dir)\n",
        "    student_tokenizer.save_pretrained(output_dir)\n",
        "    print(f\"Modelo destilado salvo em {output_dir}\")\n",
        "\n",
        "# --- Técnica 1.2: Destilação de Explicações ---\n",
        "def generate_explanation_dataset(teacher_model, tokenizer, transfer_dataset):\n",
        "    \"\"\"Usa o modelo professor para gerar um dataset com raciocínios.\"\"\"\n",
        "    print(\"\\n--- Generating Explanation-Augmented Dataset ---\")\n",
        "    prompt_template = \"A partir do contexto, responda à pergunta. Explique seu raciocínio passo a passo e termine com a resposta final. Formato: [RACIOCÍNIO] ... <sep> [RESPOSTA] ...\\n\\nContexto: {context}\\nPergunta: {question}\"\n",
        "    new_data = {\"text\": []}\n",
        "    for example in tqdm(transfer_dataset.select(range(200)), desc=\"Generating Explanations\"):\n",
        "        full_prompt = prompt_template.format(context=example['context'], question=example['question'])\n",
        "        inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(KDConfig.DEVICE)\n",
        "        outputs = teacher_model.generate(**inputs, max_new_tokens=256, pad_token_id=tokenizer.eos_token_id)\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        new_data[\"text\"].append(generated_text)\n",
        "    return Dataset.from_dict(new_data)\n",
        "\n",
        "def run_explanation_kd_training(student_model, tokenizer, explanation_dataset, config, output_dir):\n",
        "    \"\"\"Treina o estudante para gerar as explicações do professor.\"\"\"\n",
        "    print(f\"\\n--- Running Explanation Distillation -> saving to {output_dir} ---\")\n",
        "    lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\", task_type=TaskType.CAUSAL_LM)\n",
        "    peft_student_model = get_peft_model(student_model, lora_config)\n",
        "    training_args = TrainingArguments(output_dir=\"./results/explanation_training\", num_train_epochs=config.NUM_EPOCHS, learning_rate=config.LEARNING_RATE, report_to=\"none\")\n",
        "    trainer = Trainer(model=peft_student_model, args=training_args, train_dataset=explanation_dataset, tokenizer=tokenizer, data_collator=lambda data: {'input_ids': torch.stack([f['input_ids'] for f in data]), 'attention_mask': torch.stack([f['attention_mask'] for f in data]), 'labels': torch.stack([f['input_ids'] for f in data])})\n",
        "    trainer.train()\n",
        "    peft_student_model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "    print(f\"Modelo de explicação destilada salvo em {output_dir}\")"
      ],
      "metadata": {
        "id": "kd_functions"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Funções da Fase 2: Avaliação com Engenharia de Prompt"
      ],
      "metadata": {
        "id": "markdown_phase_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_zero_shot_baseline(model, tokenizer, evaluation_dataset, metrics_calculator):\n",
        "    \"\"\"Executa a avaliação de baseline (Zero-Shot Simples).\"\"\"\n",
        "    prompt = \"Contexto: {context}\\n\\nPergunta: {question}\\n\\nResposta:\"\n",
        "    return run_generic_evaluation(model, tokenizer, evaluation_dataset, metrics_calculator, prompt, \"Zero-Shot Baseline\")\n",
        "\n",
        "def run_few_shot_icl(model, tokenizer, evaluation_dataset, icl_examples, num_shots, metrics_calculator):\n",
        "    \"\"\"Executa a avaliação com In-Context Learning (Few-Shot).\"\"\"\n",
        "    prefix = \"\".join([f\"Contexto: {ex['context']}\\nPergunta: {ex['question']}\\nResposta: {ex['answers']['text'][0]}\\n\\n---\\n\\n\" for ex in icl_examples.select(range(num_shots))])\n",
        "    prompt = prefix + \"Contexto: {context}\\n\\nPergunta: {question}\\n\\nResposta:\"\n",
        "    return run_generic_evaluation(model, tokenizer, evaluation_dataset, metrics_calculator, prompt, f\"{num_shots}-Shot ICL\")\n",
        "\n",
        "def run_zero_shot_cot(model, tokenizer, evaluation_dataset, metrics_calculator):\n",
        "    \"\"\"Executa a avaliação com Zero-Shot Chain-of-Thought.\"\"\"\n",
        "    prompt = \"Contexto: {context}\\n\\nPergunta: {question}\\n\\nPense passo a passo. A resposta final é:\"\n",
        "    return run_generic_evaluation(model, tokenizer, evaluation_dataset, metrics_calculator, prompt, \"Zero-Shot CoT\")\n",
        "\n",
        "def run_self_consistency_cot(model, tokenizer, evaluation_dataset, metrics_calculator, num_paths=5):\n",
        "    \"\"\"Executa a avaliação com Auto-Consistência sobre o Zero-Shot CoT.\"\"\"\n",
        "    print(f\"\\n--- Running Self-Consistency ({num_paths} paths) ---\")\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.eval()\n",
        "    prompt_template = \"Contexto: {context}\\n\\nPergunta: {question}\\n\\nPense passo a passo. A resposta final é:\"\n",
        "\n",
        "    predictions, references = [], []\n",
        "    for example in tqdm(evaluation_dataset.select(range(20)), desc=\"Self-Consistency Eval\"):\n",
        "        prompt = prompt_template.format(context=example['context'], question=example['question'])\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        path_answers = []\n",
        "        for _ in range(num_paths):\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(**inputs, max_new_tokens=150, do_sample=True, temperature=0.7, top_k=50, pad_token_id=tokenizer.eos_token_id)\n",
        "            completion = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(prompt):].strip()\n",
        "            match = re.search(r'A resposta final é:\\s*(.*)', completion, re.IGNORECASE)\n",
        "            if match: path_answers.append(match.group(1).strip())\n",
        "\n",
        "        final_prediction = Counter(path_answers).most_common(1)[0][0] if path_answers else \"\"\n",
        "        predictions.append({'id': example['id'], 'prediction_text': final_prediction})\n",
        "        references.append({'id': example['id'], 'answers': example['answers']})\n",
        "    return metrics_calculator.compute(predictions=predictions, references=references)\n",
        "\n",
        "def run_generic_evaluation(model, tokenizer, eval_dataset, metrics_calculator, prompt_template, strategy_name, eval_subset_size=50):\n",
        "    \"\"\"Função genérica que executa a avaliação para a maioria das estratégias de prompt.\"\"\"\n",
        "    print(f\"\\n--- Running Evaluation: {strategy_name} ---\")\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.eval()\n",
        "    predictions, references = [], []\n",
        "\n",
        "    for example in tqdm(eval_dataset.select(range(eval_subset_size)), desc=f\"Evaluating {strategy_name}\"):\n",
        "        prompt = prompt_template.format(context=example['context'], question=example['question'])\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1536, truncation=True).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs, max_new_tokens=60, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "        full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        predicted_answer = full_text[len(prompt):].strip()\n",
        "\n",
        "        if \"A resposta final é:\" in prompt:\n",
        "            match = re.search(r'A resposta final é:\\s*(.*)', predicted_answer, re.IGNORECASE)\n",
        "            predicted_answer = match.group(1).strip() if match else predicted_answer.split('\\n')[-1].strip()\n",
        "\n",
        "        predictions.append({'id': example['id'], 'prediction_text': predicted_answer})\n",
        "        references.append({'id': example['id'], 'answers': example['answers']})\n",
        "\n",
        "    return metrics_calculator.compute(predictions=predictions, references=references)"
      ],
      "metadata": {
        "id": "prompting_functions"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Execução do Pipeline e Geração de Relatório (Fase 3)"
      ],
      "metadata": {
        "id": "markdown_phase_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "def run_real_workflow(run_training=False, eval_subset_size=50):\n",
        "    \"\"\"\n",
        "    Orquestra o pipeline completo e REAL: setup, treinamento (opcional), avaliação e relatório.\n",
        "\n",
        "    Args:\n",
        "        run_training (bool): Se True, executa a demorada fase de treinamento dos modelos KD.\n",
        "                             Execute como True uma vez para gerar os modelos. Depois, use False.\n",
        "        eval_subset_size (int): Número de exemplos do dataset de validação para usar na avaliação.\n",
        "                                Aumente para uma avaliação mais robusta.\n",
        "    \"\"\"\n",
        "    # --- FASE 0: SETUP ---\n",
        "    print(\"--- FASE 0: CONFIGURANDO AMBIENTE E DADOS ---\")\n",
        "    validation_data, metrics_calc = configure_task_and_metrics()\n",
        "    prepared_data = prepare_datasets(validation_data)\n",
        "    config = KDConfig()\n",
        "\n",
        "    # --- FASE 1: TREINAMENTO (Opcional) ---\n",
        "    if run_training:\n",
        "        print(\"\\n--- FASE 1: EXECUTANDO TREINAMENTO DOS MODELOS KD ---\")\n",
        "        # Carrega os modelos base necessários para todos os treinamentos\n",
        "        student_model_base, student_tokenizer = load_model(\"google/gemma-2b\")\n",
        "        teacher_model_ext, teacher_tokenizer = load_model(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
        "\n",
        "        # 1. Treina o modelo de KD Base\n",
        "        run_base_kd_training(\n",
        "            student_model=student_model_base, teacher_model=teacher_model_ext, tokenizer=tokenizer,\n",
        "            kd_dataset=prepared_data['kd_transfer'], config=config, output_dir=\"./results/kd_base_model\"\n",
        "        )\n",
        "\n",
        "        # 2. Treina o modelo de Auto-Destilação (requer um professor \"especialista\")\n",
        "        print(\"\\n--- Treinando professor para Auto-Destilação ---\")\n",
        "        # A forma mais simples de criar um professor especialista é com um fine-tuning padrão.\n",
        "        # Aqui, vamos simular isso treinando o próprio estudante com KD por uma época.\n",
        "        specialist_teacher = run_base_kd_training(\n",
        "            student_model=student_model_base, teacher_model=teacher_model_ext, tokenizer=tokenizer,\n",
        "            kd_dataset=prepared_data['kd_transfer'], config=config, output_dir=\"./results/specialist_teacher_for_self_distillation\"\n",
        "        )\n",
        "        print(\"\\n--- Executando Auto-Destilação ---\")\n",
        "        run_base_kd_training(\n",
        "            student_model=student_model_base, teacher_model=specialist_teacher, tokenizer=tokenizer,\n",
        "            kd_dataset=prepared_data['kd_transfer'], config=config, output_dir=\"./results/kd_self_distilled_model\"\n",
        "        )\n",
        "\n",
        "        # 3. Treina o modelo de Destilação de Explicações\n",
        "        explanation_dataset = generate_explanation_dataset(teacher_model_ext, tokenizer, prepared_data['kd_transfer'])\n",
        "        run_explanation_kd_training(\n",
        "            student_model=student_model_base, tokenizer=tokenizer, explanation_dataset=explanation_dataset,\n",
        "            config=config, output_dir=\"./results/explanation_distilled_model\"\n",
        "        )\n",
        "        print(\"\\n--- FASE DE TREINAMENTO CONCLUÍDA ---\")\n",
        "\n",
        "    # --- FASE 2 & 3: AVALIAÇÃO E ANÁLISE ---\n",
        "    print(\"\\n--- FASES 2 & 3: EXECUTANDO AVALIAÇÃO COMPLETA E GERANDO RELATÓRIO ---\")\n",
        "    model_definitions = {\n",
        "        \"SLM Base\": \"google/gemma-2b\",\n",
        "        \"KD Base\": \"./results/kd_base_model\",\n",
        "        \"KD Auto-Destilado\": \"./results/kd_self_distilled_model\",\n",
        "        \"KD com Explicações\": \"./results/explanation_distilled_model\"\n",
        "    }\n",
        "    prompting_strategies = {\n",
        "        \"Zero-Shot Simples\": run_zero_shot_baseline,\n",
        "        \"ICL (k=3)\": lambda m, t, d, met: run_few_shot_icl(m, t, d, prepared_data['icl_cot_examples'], 3, met),\n",
        "        \"Zero-Shot CoT\": run_zero_shot_cot,\n",
        "        \"Auto-Consistência (n=5)\": lambda m, t, d, met: run_self_consistency_cot(m, t, d, met, 5)\n",
        "    }\n",
        "    results_df = pd.DataFrame(index=model_definitions.keys(), columns=prompting_strategies.keys())\n",
        "\n",
        "    # Carrega o modelo base uma vez para aplicar os adaptadores PEFT\n",
        "    base_model_for_eval, tokenizer = load_model(\"google/gemma-2b\")\n",
        "\n",
        "    for alias, path in model_definitions.items():\n",
        "        print(f\"\\n>>>> AVALIANDO MODELO: {alias} <<<<\")\n",
        "        if alias == \"SLM Base\":\n",
        "            model = base_model_for_eval\n",
        "        else:\n",
        "            try:\n",
        "                # Carrega o modelo base com o adaptador LoRA treinado\n",
        "                model = PeftModel.from_pretrained(base_model_for_eval, path)\n",
        "                model = model.merge_and_unload() # Opcional: mescla os pesos para acelerar a inferência\n",
        "            except Exception as e:\n",
        "                print(f\"Não foi possível carregar o modelo treinado de '{path}'. Pulando. Erro: {e}\")\n",
        "                continue\n",
        "\n",
        "        for strat_name, strat_func in prompting_strategies.items():\n",
        "            # A função de avaliação genérica agora é chamada por suas wrappers específicas\n",
        "            results = strat_func(model, tokenizer, prepared_data['evaluation'], metrics_calc)\n",
        "            results_df.loc[alias, strat_name] = f\"{results['f1']:.1f} / {results['exact_match']:.1f}\"\n",
        "\n",
        "    # --- RELATÓRIO FINAL ---\n",
        "    print(\"\\n\\n## Relatório Final de Análise ##\")\n",
        "    numeric_df = results_df.applymap(lambda x: float(x.split('/')[0]) if isinstance(x, str) else 0)\n",
        "\n",
        "    print(\"\\n--- Matriz de Resultados (F1-Score / Exact Match) ---\")\n",
        "    print(results_df.to_markdown())\n",
        "\n",
        "    print(\"\\n--- Conclusões da Análise ---\")\n",
        "    avg_kd_performance = numeric_df.drop(\"SLM Base\").mean(axis=1)\n",
        "    print(f\"- Melhor Técnica de KD (Média F1): '{avg_kd_performance.idxmax()}' ({avg_kd_performance.max():.2f})\")\n",
        "    avg_prompt_performance = numeric_df.mean(axis=0)\n",
        "    print(f\"- Melhor Estratégia de Prompting (Média F1): '{avg_prompt_performance.idxmax()}' ({avg_prompt_performance.max():.2f})\")\n",
        "    best_combo = numeric_df.stack().idxmax()\n",
        "    print(f\"- Melhor Sinergia (Modelo + Prompt): '{best_combo[0]}' + '{best_combo[1]}' com F1 de {numeric_df.max().max():.2f}\")\n",
        "\n",
        "    print(\"\\nGerando gráfico de comparação...\")\n",
        "    fig, ax = plt.subplots(figsize=(12, 7))\n",
        "    numeric_df.plot(kind='bar', ax=ax, title='Comparação de Performance (F1-Score) entre Modelos e Estratégias')\n",
        "    ax.set_ylabel(\"F1-Score\")\n",
        "    ax.set_xlabel(\"Versão do Modelo\")\n",
        "    ax.tick_params(axis='x', rotation=20)\n",
        "    plt.legend(title='Estratégia de Prompt', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"real_run_full_performance_comparison.png\")\n",
        "    print(\"Gráfico salvo em 'real_run_full_performance_comparison.png'.\")"
      ],
      "metadata": {
        "id": "uMnh8PID06Wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Execução do Pipeline (Modo Simulado)\n",
        "\n",
        "A célula abaixo executa o pipeline em **modo de simulação**. Ela não treina nem avalia os modelos de verdade, mas gera um relatório com dados de exemplo para demonstrar a estrutura de análise. Para uma execução real, você deve primeiro executar as células de treinamento da Fase 1 e depois rodar o pipeline com `simulation_mode=False`."
      ],
      "metadata": {
        "id": "markdown_run_pipeline"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: login hugging face\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "5d46296f695b44b19e3b10ed534e7d8c",
            "02e745a6dc56472baae2ef5733b1c63d",
            "2336710c68854cca889a9142efd81825",
            "97eb7ce9f5fd4f128e15724be5fe74aa",
            "c88da374e42a4ea79f0eb2715fe24a0f",
            "bbc948efb0734724ab7ccd56654b05f1",
            "2e746c17055c4d319db73aea6ffcf91d",
            "cd6bed10f72a4abe87d5f238595a1504",
            "abab12c35fc14549a0f854f343a5028e",
            "d556d17b4043458d9b6a32bf1a53fa95",
            "fea39897cab94ccab0d7ac7151873a52",
            "6f2e5ccad8bf4ce4be9d061ee67a9740",
            "7320592180ad4a188f0020db514d4ece",
            "ecbbaf48b56a4620b2a211ee4392d5a9",
            "26136d029e274b6e99bc71ff7c7f86ed",
            "0dab0e4275124a23b9b5dce2d3a53c54",
            "fd8e2425c16142e2a64ad65fb9c7846d",
            "11adbbe5a15b4a78934d8281fa40b0e1",
            "c5f57d4a83ba4303957af3a39f885aa9",
            "f237b8dcca8f4c14a8226b09c5462f8f"
          ]
        },
        "id": "Nwo8_KdnLleN",
        "outputId": "bf52cdc3-043a-4227-e463-c8b1b965a54d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5d46296f695b44b19e3b10ed534e7d8c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_full_pipeline(simulation_mode=True)"
      ],
      "metadata": {
        "id": "run-pipeline",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "798bbdcf-9b60-47b9-df11-04557516311b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'run_full_pipeline' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-51570dfb6e7b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_full_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimulation_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'run_full_pipeline' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "def run_real_workflow(run_training=False, eval_subset_size=50):\n",
        "    \"\"\"\n",
        "    Orquestra o pipeline completo e REAL: setup, treinamento (opcional), avaliação e relatório.\n",
        "\n",
        "    Args:\n",
        "        run_training (bool): Se True, executa a demorada fase de treinamento dos modelos KD.\n",
        "                             Execute como True uma vez para gerar os modelos. Depois, use False.\n",
        "        eval_subset_size (int): Número de exemplos do dataset de validação para usar na avaliação.\n",
        "                                Aumente para uma avaliação mais robusta.\n",
        "    \"\"\"\n",
        "    # --- FASE 0: SETUP ---\n",
        "    print(\"--- FASE 0: CONFIGURANDO AMBIENTE E DADOS ---\")\n",
        "    validation_data, metrics_calc = configure_task_and_metrics()\n",
        "    prepared_data = prepare_datasets(validation_data)\n",
        "    config = KDConfig()\n",
        "\n",
        "    # --- FASE 1: TREINAMENTO (Opcional) ---\n",
        "    if run_training:\n",
        "        print(\"\\n--- FASE 1: EXECUTANDO TREINAMENTO DOS MODELOS KD ---\")\n",
        "        # Carrega os modelos base necessários para todos os treinamentos\n",
        "        student_model_base, student_tokenizer = load_model(\"google/gemma-2b\")\n",
        "        teacher_model_ext, teacher_tokenizer = load_model(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
        "        print(teacher_model_ext)\n",
        "\n",
        "        # 1. Treina o modelo de KD Base\n",
        "        run_base_kd_training(\n",
        "            student_model=student_model_base, teacher_model=teacher_model_ext, student_tokenizer=student_tokenizer, teacher_tokenizer=teacher_tokenizer,\n",
        "            kd_dataset=prepared_data['kd_transfer'], config=config, output_dir=\"./results/kd_base_model\"\n",
        "        )\n",
        "\n",
        "        # 2. Treina o modelo de Auto-Destilação (requer um professor \"especialista\")\n",
        "        print(\"\\n--- Treinando professor para Auto-Destilação ---\")\n",
        "        # A forma mais simples de criar um professor especialista é com um fine-tuning padrão.\n",
        "        # Aqui, vamos simular isso treinando o próprio estudante com KD por uma época.\n",
        "        specialist_teacher = run_base_kd_training(\n",
        "            student_model=student_model_base, teacher_model=teacher_model_ext, student_tokenizer=student_tokenizer, teacher_tokenizer=teacher_tokenizer,\n",
        "            kd_dataset=prepared_data['kd_transfer'], config=config, output_dir=\"./results/specialist_teacher_for_self_distillation\"\n",
        "        )\n",
        "        print(\"\\n--- Executando Auto-Destilação ---\")\n",
        "        run_base_kd_training(\n",
        "            student_model=student_model_base, teacher_model=specialist_teacher, student_tokenizer=student_tokenizer, teacher_tokenizer=teacher_tokenizer,\n",
        "            kd_dataset=prepared_data['kd_transfer'], config=config, output_dir=\"./results/kd_self_distilled_model\"\n",
        "        )\n",
        "\n",
        "        # 3. Treina o modelo de Destilação de Explicações\n",
        "        explanation_dataset = generate_explanation_dataset(teacher_model_ext, teacher_tokenizer, prepared_data['kd_transfer'])\n",
        "        run_explanation_kd_training(\n",
        "            student_model=student_model_base, student_tokenizer=student_tokenizer, teacher_tokenizer=teacher_tokenizer, explanation_dataset=explanation_dataset,\n",
        "            config=config, output_dir=\"./results/explanation_distilled_model\"\n",
        "        )\n",
        "        print(\"\\n--- FASE DE TREINAMENTO CONCLUÍDA ---\")\n",
        "\n",
        "    # --- FASE 2 & 3: AVALIAÇÃO E ANÁLISE ---\n",
        "    print(\"\\n--- FASES 2 & 3: EXECUTANDO AVALIAÇÃO COMPLETA E GERANDO RELATÓRIO ---\")\n",
        "    model_definitions = {\n",
        "        \"SLM Base\": \"google/gemma-2b\",\n",
        "        \"KD Base\": \"./results/kd_base_model\",\n",
        "        \"KD Auto-Destilado\": \"./results/kd_self_distilled_model\",\n",
        "        \"KD com Explicações\": \"./results/explanation_distilled_model\"\n",
        "    }\n",
        "    prompting_strategies = {\n",
        "        \"Zero-Shot Simples\": run_zero_shot_baseline,\n",
        "        \"ICL (k=3)\": lambda m, t, d, met: run_few_shot_icl(m, t, d, prepared_data['icl_cot_examples'], 3, met),\n",
        "        \"Zero-Shot CoT\": run_zero_shot_cot,\n",
        "        \"Auto-Consistência (n=5)\": lambda m, t, d, met: run_self_consistency_cot(m, t, d, met, 5)\n",
        "    }\n",
        "    results_df = pd.DataFrame(index=model_definitions.keys(), columns=prompting_strategies.keys())\n",
        "\n",
        "    # Carrega o modelo base uma vez para aplicar os adaptadores PEFT\n",
        "    base_model_for_eval, tokenizer = load_model(\"google/gemma-2b\")\n",
        "\n",
        "    for alias, path in model_definitions.items():\n",
        "        print(f\"\\n>>>> AVALIANDO MODELO: {alias} <<<<\")\n",
        "        if alias == \"SLM Base\":\n",
        "            model = base_model_for_eval\n",
        "        else:\n",
        "            try:\n",
        "                # Carrega o modelo base com o adaptador LoRA treinado\n",
        "                model = PeftModel.from_pretrained(base_model_for_eval, path)\n",
        "                model = model.merge_and_unload() # Opcional: mescla os pesos para acelerar a inferência\n",
        "            except Exception as e:\n",
        "                print(f\"Não foi possível carregar o modelo treinado de '{path}'. Pulando. Erro: {e}\")\n",
        "                continue\n",
        "\n",
        "        for strat_name, strat_func in prompting_strategies.items():\n",
        "            # A função de avaliação genérica agora é chamada por suas wrappers específicas\n",
        "            results = strat_func(model, tokenizer, prepared_data['evaluation'], metrics_calc)\n",
        "            results_df.loc[alias, strat_name] = f\"{results['f1']:.1f} / {results['exact_match']:.1f}\"\n",
        "\n",
        "    # --- RELATÓRIO FINAL ---\n",
        "    print(\"\\n\\n## Relatório Final de Análise ##\")\n",
        "    numeric_df = results_df.applymap(lambda x: float(x.split('/')[0]) if isinstance(x, str) else 0)\n",
        "\n",
        "    print(\"\\n--- Matriz de Resultados (F1-Score / Exact Match) ---\")\n",
        "    print(results_df.to_markdown())\n",
        "\n",
        "    print(\"\\n--- Conclusões da Análise ---\")\n",
        "    avg_kd_performance = numeric_df.drop(\"SLM Base\").mean(axis=1)\n",
        "    print(f\"- Melhor Técnica de KD (Média F1): '{avg_kd_performance.idxmax()}' ({avg_kd_performance.max():.2f})\")\n",
        "    avg_prompt_performance = numeric_df.mean(axis=0)\n",
        "    print(f\"- Melhor Estratégia de Prompting (Média F1): '{avg_prompt_performance.idxmax()}' ({avg_prompt_performance.max():.2f})\")\n",
        "    best_combo = numeric_df.stack().idxmax()\n",
        "    print(f\"- Melhor Sinergia (Modelo + Prompt): '{best_combo[0]}' + '{best_combo[1]}' com F1 de {numeric_df.max().max():.2f}\")\n",
        "\n",
        "    print(\"\\nGerando gráfico de comparação...\")\n",
        "    fig, ax = plt.subplots(figsize=(12, 7))\n",
        "    numeric_df.plot(kind='bar', ax=ax, title='Comparação de Performance (F1-Score) entre Modelos e Estratégias')\n",
        "    ax.set_ylabel(\"F1-Score\")\n",
        "    ax.set_xlabel(\"Versão do Modelo\")\n",
        "    ax.tick_params(axis='x', rotation=20)\n",
        "    plt.legend(title='Estratégia de Prompt', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"real_run_full_performance_comparison.png\")\n",
        "    print(\"Gráfico salvo em 'real_run_full_performance_comparison.png'.\")"
      ],
      "metadata": {
        "id": "7qySGyuBLwrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_real_workflow(True, 10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426,
          "referenced_widgets": [
            "98b304a4e8bd45488839877687d4c346",
            "4a9a3b739a9746268b8935f17bea382c",
            "85a4be3f0db44665b23c7c7b5325ea03",
            "45ef3133611e4da4ba69c6a80a0b9860",
            "52db1df8091d46928516be4eebc7f5ca",
            "dc793fffa9a74823b4c250a897e6c80e",
            "ffc3adac99a8435dba04c494e41118e9",
            "3bf241642ed04c999d95c681e5cf1df5",
            "d01196581b804d2781ca3c88a955228e",
            "2b1c3c2352a44df7a6da5f96f015d6c0",
            "519d719e80b64aa7a7021aa609dfe8e8",
            "c2a38335034b49e093213144bcfa0d31",
            "5f9a19b6335c40e78a095b4fe7f57667",
            "e75bb0f9a87445d3a025a0b2f4a2ddbc",
            "53459e5db4214daa86d6e97113088e82",
            "f127d4bc0b8e4b3a97569ebdc7c9952b",
            "f6671d0a74a446f4bbb7d8bb4263e8de",
            "46443d1167554710b82fdbd299fa65b0",
            "ef69db249a4344c3946903d5ae2a4ce6",
            "4c35eab1b8cc49b38e0258a2cbf9a487",
            "53817bec58e04feb88aeaead1249c37e",
            "6788985ac10f4087b814ad71ee9f713d"
          ]
        },
        "id": "e2zr_LJsNAIM",
        "outputId": "5af55386-bdba-4586-c911-d3d2e359c661"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- FASE 0: CONFIGURANDO AMBIENTE E DADOS ---\n",
            "--- Step 0.1: Configuring Task (SQuAD) and Metrics ---\n",
            "Dataset e métricas carregados.\n",
            "\n",
            "--- Step 0.4: Preparing Datasets ---\n",
            "Datasets preparados.\n",
            "\n",
            "--- FASE 1: EXECUTANDO TREINAMENTO DOS MODELOS KD ---\n",
            "\n",
            "--- Loading Model: google/gemma-2b ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "98b304a4e8bd45488839877687d4c346"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo carregado com sucesso.\n",
            "\n",
            "--- Loading Model: mistralai/Mistral-7B-Instruct-v0.2 ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2a38335034b49e093213144bcfa0d31"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Erro ao carregar o modelo: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "ERRO CRÍTICO: Falha ao carregar um dos modelos base. O pipeline não pode continuar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FASE 0: SETUP ---\n",
        "print(\"--- FASE 0: CONFIGURANDO AMBIENTE E DADOS ---\")\n",
        "validation_data, metrics_calc = configure_task_and_metrics()\n",
        "prepared_data = prepare_datasets(validation_data)\n",
        "config = KDConfig()\n",
        "\n",
        "run_training  = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-exjfMDD_8F",
        "outputId": "cbf4f494-a5fa-4928-fb76-a981d5bbee44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- FASE 0: CONFIGURANDO AMBIENTE E DADOS ---\n",
            "--- Step 0.1: Configuring Task (SQuAD) and Metrics ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset e métricas carregados.\n",
            "\n",
            "--- Step 0.4: Preparing Datasets ---\n",
            "Datasets preparados.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FASE 1: TREINAMENTO (Opcional) ---\n",
        "\n",
        "print(\"\\n--- FASE 1: EXECUTANDO TREINAMENTO DOS MODELOS KD ---\")\n",
        "# Carrega os modelos base necessários para todos os treinamentos\n",
        "student_model_base, student_tokenizer = load_model(\"google/gemma-2b\")\n",
        "teacher_model_ext, teacher_tokenizer = load_model(\"mistralai/Mistral-7B-Instruct-v0.2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244,
          "referenced_widgets": [
            "f7313eb452b945abaac6b64ead90e703",
            "645a6963efa74f999df3421091f6b5b7",
            "ccd4201b231741c3aa4dd5a18ff961b1",
            "11b58d107b4d4a5fb44a5e01046e1bd9",
            "51b0b23c7ad449c5a4631e8d2c592ef4",
            "be6b6fb0f4c9433198bbf7cbca5c7874",
            "fe2d2f2b05324dfbbec4e813f8aeb2ab",
            "ebcd3a71ad9f444da9fe71e3db1adabe",
            "399ddfc042e9492a85bbdf7cc348cc23",
            "4eebd8df447540ecbcc9287ee34fbc94",
            "1c524b94325342e893e48ab4da3139e9",
            "1e6ba4e3ef3a446aa9bfe53291ac1562",
            "6dc30bdfb5b54ed0b8743d2362181256",
            "ec0e17f63be444fbab699dfa3e723bf3",
            "7ac611c0386d47f0b2c5231336da029e",
            "40c386b42d4d4fb18d297b4dc1df1bd5",
            "442f704806e448dcbfb1d5271f1fa2ab",
            "60c2c4506ccd4995b813ae2980f04e28",
            "b2d91432b63042a3a6191ce686f1edcd",
            "e8fee87eef93433a9124f48ad040666e",
            "2ecf1a0bc6af42bfb6969936fc757a23",
            "20bf94add42f468e93015845e4fd2a72"
          ]
        },
        "id": "hoj0SmeaELER",
        "outputId": "2b42eada-fdd7-4304-8b78-7266afc37c67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- FASE 1: EXECUTANDO TREINAMENTO DOS MODELOS KD ---\n",
            "\n",
            "--- Loading Model: google/gemma-2b ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f7313eb452b945abaac6b64ead90e703"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo carregado com sucesso.\n",
            "\n",
            "--- Loading Model: mistralai/Mistral-7B-Instruct-v0.2 ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e6ba4e3ef3a446aa9bfe53291ac1562"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo carregado com sucesso.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Treina o modelo de KD Base\n",
        "run_base_kd_training(\n",
        "    student_model=student_model_base, teacher_model=teacher_model_ext, student_tokenizer=student_tokenizer, teacher_tokenizer=teacher_tokenizer,\n",
        "    kd_dataset=prepared_data['kd_transfer'], config=config, output_dir=\"./results/kd_base_model\"\n",
        ")\n",
        "\n",
        "# 2. Treina o modelo de Auto-Destilação (requer um professor \"especialista\")\n",
        "print(\"\\n--- Treinando professor para Auto-Destilação ---\")\n",
        "# A forma mais simples de criar um professor especialista é com um fine-tuning padrão.\n",
        "# Aqui, vamos simular isso treinando o próprio estudante com KD por uma época.\n",
        "specialist_teacher = run_base_kd_training(\n",
        "    student_model=student_model_base, teacher_model=teacher_model_ext, student_tokenizer=student_tokenizer, teacher_tokenizer=teacher_tokenizer,\n",
        "    kd_dataset=prepared_data['kd_transfer'], config=config, output_dir=\"./results/specialist_teacher_for_self_distillation\"\n",
        ")\n",
        "print(\"\\n--- Executando Auto-Destilação ---\")\n",
        "run_base_kd_training(\n",
        "    student_model=student_model_base, teacher_model=specialist_teacher, student_tokenizer=student_tokenizer, teacher_tokenizer=teacher_tokenizer,\n",
        "    kd_dataset=prepared_data['kd_transfer'], config=config, output_dir=\"./results/kd_self_distilled_model\"\n",
        ")\n",
        "\n",
        "# 3. Treina o modelo de Destilação de Explicações\n",
        "explanation_dataset = generate_explanation_dataset(teacher_model_ext, teacher_tokenizer, prepared_data['kd_transfer'])\n",
        "run_explanation_kd_training(\n",
        "    student_model=student_model_base, student_tokenizer=student_tokenizer, teacher_tokenizer=teacher_tokenizer, explanation_dataset=explanation_dataset,\n",
        "    config=config, output_dir=\"./results/explanation_distilled_model\"\n",
        ")\n",
        "print(\"\\n--- FASE DE TREINAMENTO CONCLUÍDA ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529,
          "referenced_widgets": [
            "ab477ce71f0d4569b6efe5e6e2b6fd03",
            "fba007a9e58a422aabe9cdc2e911fb11",
            "f53f017bf5924d3698754356c29fd7a1",
            "28a01690abed47d1aff9f3c410679ce4",
            "bfd008ed77ea43cdbc9a2aee0c84da0e",
            "1f69c06071ec44ad89795297e416a3dd",
            "6477b0805ac14c4eb7a21380cd02da9d",
            "e553017af8ef42a3b3335ab74164ab5b",
            "4dff25c9975441d395b9d4b9e582b08b",
            "024ed6944a2f4263b2903ecdf03f05c4",
            "8b7d193a6b844813b84ab3404fc95677"
          ]
        },
        "id": "I9mMGD2IE1Xw",
        "outputId": "bee6b90c-a45e-47ac-f4fa-87d0bdc951d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Running Base Knowledge Distillation -> saving to ./results/kd_base_model ---\n",
            "\n",
            "Epoch 1/1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 1:   0%|          | 0/200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab477ce71f0d4569b6efe5e6e2b6fd03"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 367409 has 14.73 GiB memory in use. Of the allocated memory 14.58 GiB is allocated by PyTorch, and 30.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-8ce78d310d2a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 1. Treina o modelo de KD Base\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m run_base_kd_training(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mstudent_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstudent_model_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mteacher_model_ext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudent_tokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstudent_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_tokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mteacher_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mkd_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'kd_transfer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./results/kd_base_model\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;32m<ipython-input-6-ff7514fb43f6>\u001b[0m in \u001b[0;36mrun_base_kd_training\u001b[0;34m(student_model, teacher_model, student_tokenizer, teacher_tokenizer, kd_dataset, config, output_dir)\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mteacher_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mteacher_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mteacher_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mstudent_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpeft_student_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mstudent_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# A perda é calculada entre os logits, que devem ter o mesmo tamanho de vocabulário\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1755\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1756\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m                 return self.base_model(\n\u001b[0m\u001b[1;32m   1758\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1759\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma/modeling_gemma.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    684\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma/modeling_gemma.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    450\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_checkpointing\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma/modeling_gemma.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma/modeling_gemma.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdown_proj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnew_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mpre_forward\u001b[0;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtied_pointers_to_remove\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m                 set_module_tensor_to_device(\n\u001b[0m\u001b[1;32m    361\u001b[0m                     \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                     \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/modeling.py\u001b[0m in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    335\u001b[0m                     \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mold_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0mnew_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0mnew_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 367409 has 14.73 GiB memory in use. Of the allocated memory 14.58 GiB is allocated by PyTorch, and 30.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FASE 2 & 3: AVALIAÇÃO E ANÁLISE ---\n",
        "print(\"\\n--- FASES 2 & 3: EXECUTANDO AVALIAÇÃO COMPLETA E GERANDO RELATÓRIO ---\")\n",
        "model_definitions = {\n",
        "    \"SLM Base\": \"google/gemma-2b\",\n",
        "    \"KD Base\": \"./results/kd_base_model\",\n",
        "    \"KD Auto-Destilado\": \"./results/kd_self_distilled_model\",\n",
        "    \"KD com Explicações\": \"./results/explanation_distilled_model\"\n",
        "}\n",
        "prompting_strategies = {\n",
        "    \"Zero-Shot Simples\": run_zero_shot_baseline,\n",
        "    \"ICL (k=3)\": lambda m, t, d, met: run_few_shot_icl(m, t, d, prepared_data['icl_cot_examples'], 3, met),\n",
        "    \"Zero-Shot CoT\": run_zero_shot_cot,\n",
        "    \"Auto-Consistência (n=5)\": lambda m, t, d, met: run_self_consistency_cot(m, t, d, met, 5)\n",
        "}\n",
        "results_df = pd.DataFrame(index=model_definitions.keys(), columns=prompting_strategies.keys())"
      ],
      "metadata": {
        "id": "OrnYoUT6EOcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega o modelo base uma vez para aplicar os adaptadores PEFT\n",
        "base_model_for_eval, tokenizer = load_model(\"google/gemma-2b\")\n",
        "\n",
        "for alias, path in model_definitions.items():\n",
        "    print(f\"\\n>>>> AVALIANDO MODELO: {alias} <<<<\")\n",
        "    if alias == \"SLM Base\":\n",
        "        model = base_model_for_eval\n",
        "    else:\n",
        "        try:\n",
        "            # Carrega o modelo base com o adaptador LoRA treinado\n",
        "            model = PeftModel.from_pretrained(base_model_for_eval, path)\n",
        "            model = model.merge_and_unload() # Opcional: mescla os pesos para acelerar a inferência\n",
        "        except Exception as e:\n",
        "            print(f\"Não foi possível carregar o modelo treinado de '{path}'. Pulando. Erro: {e}\")\n",
        "            continue\n",
        "\n",
        "    for strat_name, strat_func in prompting_strategies.items():\n",
        "        # A função de avaliação genérica agora é chamada por suas wrappers específicas\n",
        "        results = strat_func(model, tokenizer, prepared_data['evaluation'], metrics_calc)\n",
        "        results_df.loc[alias, strat_name] = f\"{results['f1']:.1f} / {results['exact_match']:.1f}\"\n",
        "\n",
        "# --- RELATÓRIO FINAL ---\n",
        "print(\"\\n\\n## Relatório Final de Análise ##\")\n",
        "numeric_df = results_df.applymap(lambda x: float(x.split('/')[0]) if isinstance(x, str) else 0)\n",
        "\n",
        "print(\"\\n--- Matriz de Resultados (F1-Score / Exact Match) ---\")\n",
        "print(results_df.to_markdown())\n",
        "\n",
        "print(\"\\n--- Conclusões da Análise ---\")\n",
        "avg_kd_performance = numeric_df.drop(\"SLM Base\").mean(axis=1)\n",
        "print(f\"- Melhor Técnica de KD (Média F1): '{avg_kd_performance.idxmax()}' ({avg_kd_performance.max():.2f})\")\n",
        "avg_prompt_performance = numeric_df.mean(axis=0)\n",
        "print(f\"- Melhor Estratégia de Prompting (Média F1): '{avg_prompt_performance.idxmax()}' ({avg_prompt_performance.max():.2f})\")\n",
        "best_combo = numeric_df.stack().idxmax()\n",
        "print(f\"- Melhor Sinergia (Modelo + Prompt): '{best_combo[0]}' + '{best_combo[1]}' com F1 de {numeric_df.max().max():.2f}\")\n",
        "\n",
        "print(\"\\nGerando gráfico de comparação...\")\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "numeric_df.plot(kind='bar', ax=ax, title='Comparação de Performance (F1-Score) entre Modelos e Estratégias')\n",
        "ax.set_ylabel(\"F1-Score\")\n",
        "ax.set_xlabel(\"Versão do Modelo\")\n",
        "ax.tick_params(axis='x', rotation=20)\n",
        "plt.legend(title='Estratégia de Prompt', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"real_run_full_performance_comparison.png\")\n",
        "print(\"Gráfico salvo em 'real_run_full_performance_comparison.png'.\")"
      ],
      "metadata": {
        "id": "JXVjH7h1Ede_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}