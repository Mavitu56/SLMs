{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPi0n3MzPZdk9Axh+uvixAd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mavitu56/SLMs/blob/main/SLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Workflow Completo: Aprimoramento e Comparação de SLMs\n",
        "\n",
        "Este notebook é a versão final, completa e funcional do plano para aprimorar e comparar um Small Language Model (SLM) usando Destilação de Conhecimento e Engenharia de Prompt."
      ],
      "metadata": {
        "id": "markdown_intro_main"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Instalação e Importações Globais"
      ],
      "metadata": {
        "id": "markdown_deps"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hg4t9Rbq06Wq"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers \"datasets==2.19.0\" evaluate peft accelerate ipywidgets sentencepiece pandas matplotlib --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import re\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from collections import Counter\n",
        "from tqdm.notebook import tqdm\n",
        "from datasets import load_dataset, Dataset\n",
        "from evaluate import load\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from torch.optim import AdamW\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "imports-central"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Funções da Fase 0: Configuração do Ambiente"
      ],
      "metadata": {
        "id": "markdown_phase_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def configure_task_and_metrics():\n",
        "    \"\"\"Carrega o dataset e as métricas para a tarefa de avaliação.\"\"\"\n",
        "    print(\"--- Step 0.1: Configuring Task (SQuAD) and Metrics ---\")\n",
        "    validation_dataset = load_dataset(\"squad\", split=\"validation\")\n",
        "    squad_metric = load(\"squad\")\n",
        "    print(\"Dataset e métricas carregados.\")\n",
        "    return validation_dataset, squad_metric\n",
        "\n",
        "def load_model(model_id):\n",
        "    \"\"\"Carrega um modelo e seu tokenizador a partir de um ID.\"\"\"\n",
        "    print(f\"\\n--- Loading Model: {model_id} ---\")\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        # Adiciona um pad token se não existir, comum para modelos de geração\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id, device_map=\"auto\", torch_dtype=torch.bfloat16\n",
        "        )\n",
        "        print(\"Modelo carregado com sucesso.\")\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao carregar o modelo: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def prepare_datasets(validation_dataset):\n",
        "    \"\"\"Prepara todos os subconjuntos de dados necessários.\"\"\"\n",
        "    print(\"\\n--- Step 0.4: Preparing Datasets ---\")\n",
        "    squad_train_full = load_dataset(\"squad\", split=\"train\")\n",
        "    # Usamos um subset menor para o KD Transfer para agilizar o treinamento\n",
        "    kd_transfer_set = squad_train_full.shuffle(seed=42).select(range(10000))\n",
        "    icl_cot_set = squad_train_full.select(range(5))\n",
        "    datasets = {\n",
        "        \"evaluation\": validation_dataset,\n",
        "        \"kd_transfer\": kd_transfer_set,\n",
        "        \"icl_cot_examples\": icl_cot_set\n",
        "    }\n",
        "    print(\"Datasets preparados.\")\n",
        "    return datasets"
      ],
      "metadata": {
        "id": "4Hlnxt1s06Wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Funções da Fase 1: Destilação de Conhecimento (KD)"
      ],
      "metadata": {
        "id": "markdown_phase_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KDConfig:\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    TEMPERATURE = 2.0\n",
        "    LEARNING_RATE = 5e-5\n",
        "    NUM_EPOCHS = 1 # Reduzido para uma execução mais rápida de exemplo. Aumente para 3 em um treino real.\n",
        "\n",
        "# --- Técnica 1.1: KD Base (Logit Matching) ---\n",
        "def compute_distillation_loss(student_logits, teacher_logits, temperature):\n",
        "    \"\"\"Calcula a perda de Destilação de Conhecimento (KL Divergence).\"\"\"\n",
        "    soft_teacher_probs = F.softmax(teacher_logits / temperature, dim=-1)\n",
        "    soft_student_log_probs = F.log_softmax(student_logits / temperature, dim=-1)\n",
        "    distillation_loss = F.kl_div(soft_student_log_probs, soft_teacher_probs, reduction='batchmean')\n",
        "    return (temperature**2) * distillation_loss\n",
        "\n",
        "def run_base_kd_training(student_model, teacher_model, student_tokenizer, teacher_tokenizer, kd_dataset, config, output_dir):\n",
        "    \"\"\"Executa o loop de treinamento para a Destilação de Conhecimento base.\"\"\"\n",
        "    print(f\"\\n--- Running Base Knowledge Distillation -> saving to {output_dir} ---\")\n",
        "    lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\", task_type=TaskType.CAUSAL_LM)\n",
        "    peft_student_model = get_peft_model(student_model, lora_config)\n",
        "    optimizer = AdamW(peft_student_model.parameters(), lr=config.LEARNING_RATE)\n",
        "    teacher_model.eval()\n",
        "\n",
        "    for epoch in range(config.NUM_EPOCHS):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{config.NUM_EPOCHS}\")\n",
        "        peft_student_model.train()\n",
        "        for batch in tqdm(kd_dataset.shuffle(seed=epoch).select(range(200)), desc=f\"Epoch {epoch+1}\"):\n",
        "            prompt = f\"Contexto: {batch['context']}\\n\\nPergunta: {batch['question']}\"\n",
        "\n",
        "            # Tokeniza para o PROFESSOR com o tokenizador do PROFESSOR\n",
        "            teacher_inputs = teacher_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(config.DEVICE)\n",
        "\n",
        "            # Tokeniza para o ESTUDANTE com o tokenizador do ESTUDANTE\n",
        "            student_inputs = student_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(config.DEVICE)\n",
        "\n",
        "            # Gera os logits de cada modelo com seus respectivos inputs\n",
        "            with torch.no_grad():\n",
        "                teacher_logits = teacher_model(**teacher_inputs).logits\n",
        "\n",
        "            student_logits = peft_student_model(**student_inputs).logits\n",
        "\n",
        "            # A perda é calculada entre os logits, que devem ter o mesmo tamanho de vocabulário\n",
        "            # Se os vocabulários forem diferentes, a destilação de logits não é diretamente aplicável\n",
        "            # e a \"Destilação de Explicações\" se torna a abordagem correta.\n",
        "            # Adicionando uma verificação para evitar o erro:\n",
        "            if student_logits.shape[-1] != teacher_logits.shape[-1]:\n",
        "                raise ValueError(\n",
        "                    f\"Os tamanhos dos vocabulários do estudante ({student_logits.shape[-1]}) e do professor ({teacher_logits.shape[-1]}) são diferentes. \"\n",
        "                    \"A destilação de logits direta não é possível. Use a Destilação de Explicações.\"\n",
        "                )\n",
        "\n",
        "            loss = compute_distillation_loss(student_logits, teacher_logits, config.TEMPERATURE)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "    peft_student_model.save_pretrained(output_dir)\n",
        "    student_tokenizer.save_pretrained(output_dir)\n",
        "    print(f\"Modelo destilado salvo em {output_dir}\")\n",
        "\n",
        "# --- Técnica 1.2: Destilação de Explicações ---\n",
        "def generate_explanation_dataset(teacher_model, tokenizer, transfer_dataset):\n",
        "    \"\"\"Usa o modelo professor para gerar um dataset com raciocínios.\"\"\"\n",
        "    print(\"\\n--- Generating Explanation-Augmented Dataset ---\")\n",
        "    prompt_template = \"A partir do contexto, responda à pergunta. Explique seu raciocínio passo a passo e termine com a resposta final. Formato: [RACIOCÍNIO] ... <sep> [RESPOSTA] ...\\n\\nContexto: {context}\\nPergunta: {question}\"\n",
        "    new_data = {\"text\": []}\n",
        "    for example in tqdm(transfer_dataset.select(range(200)), desc=\"Generating Explanations\"):\n",
        "        full_prompt = prompt_template.format(context=example['context'], question=example['question'])\n",
        "        inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(KDConfig.DEVICE)\n",
        "        outputs = teacher_model.generate(**inputs, max_new_tokens=256, pad_token_id=tokenizer.eos_token_id)\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        new_data[\"text\"].append(generated_text)\n",
        "    return Dataset.from_dict(new_data)\n",
        "\n",
        "def run_explanation_kd_training(student_model, tokenizer, explanation_dataset, config, output_dir):\n",
        "    \"\"\"Treina o estudante para gerar as explicações do professor.\"\"\"\n",
        "    print(f\"\\n--- Running Explanation Distillation -> saving to {output_dir} ---\")\n",
        "    lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\", task_type=TaskType.CAUSAL_LM)\n",
        "    peft_student_model = get_peft_model(student_model, lora_config)\n",
        "    training_args = TrainingArguments(output_dir=\"./results/explanation_training\", num_train_epochs=config.NUM_EPOCHS, learning_rate=config.LEARNING_RATE, report_to=\"none\")\n",
        "    trainer = Trainer(model=peft_student_model, args=training_args, train_dataset=explanation_dataset, tokenizer=tokenizer, data_collator=lambda data: {'input_ids': torch.stack([f['input_ids'] for f in data]), 'attention_mask': torch.stack([f['attention_mask'] for f in data]), 'labels': torch.stack([f['input_ids'] for f in data])})\n",
        "    trainer.train()\n",
        "    peft_student_model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "    print(f\"Modelo de explicação destilada salvo em {output_dir}\")"
      ],
      "metadata": {
        "id": "kd_functions"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Funções da Fase 2: Avaliação com Engenharia de Prompt"
      ],
      "metadata": {
        "id": "markdown_phase_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_zero_shot_baseline(model, tokenizer, evaluation_dataset, metrics_calculator):\n",
        "    \"\"\"Executa a avaliação de baseline (Zero-Shot Simples).\"\"\"\n",
        "    prompt = \"Contexto: {context}\\n\\nPergunta: {question}\\n\\nResposta:\"\n",
        "    return run_generic_evaluation(model, tokenizer, evaluation_dataset, metrics_calculator, prompt, \"Zero-Shot Baseline\")\n",
        "\n",
        "def run_few_shot_icl(model, tokenizer, evaluation_dataset, icl_examples, num_shots, metrics_calculator):\n",
        "    \"\"\"Executa a avaliação com In-Context Learning (Few-Shot).\"\"\"\n",
        "    prefix = \"\".join([f\"Contexto: {ex['context']}\\nPergunta: {ex['question']}\\nResposta: {ex['answers']['text'][0]}\\n\\n---\\n\\n\" for ex in icl_examples.select(range(num_shots))])\n",
        "    prompt = prefix + \"Contexto: {context}\\n\\nPergunta: {question}\\n\\nResposta:\"\n",
        "    return run_generic_evaluation(model, tokenizer, evaluation_dataset, metrics_calculator, prompt, f\"{num_shots}-Shot ICL\")\n",
        "\n",
        "def run_zero_shot_cot(model, tokenizer, evaluation_dataset, metrics_calculator):\n",
        "    \"\"\"Executa a avaliação com Zero-Shot Chain-of-Thought.\"\"\"\n",
        "    prompt = \"Contexto: {context}\\n\\nPergunta: {question}\\n\\nPense passo a passo. A resposta final é:\"\n",
        "    return run_generic_evaluation(model, tokenizer, evaluation_dataset, metrics_calculator, prompt, \"Zero-Shot CoT\")\n",
        "\n",
        "def run_self_consistency_cot(model, tokenizer, evaluation_dataset, metrics_calculator, num_paths=5):\n",
        "    \"\"\"Executa a avaliação com Auto-Consistência sobre o Zero-Shot CoT.\"\"\"\n",
        "    print(f\"\\n--- Running Self-Consistency ({num_paths} paths) ---\")\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.eval()\n",
        "    prompt_template = \"Contexto: {context}\\n\\nPergunta: {question}\\n\\nPense passo a passo. A resposta final é:\"\n",
        "\n",
        "    predictions, references = [], []\n",
        "    for example in tqdm(evaluation_dataset.select(range(20)), desc=\"Self-Consistency Eval\"):\n",
        "        prompt = prompt_template.format(context=example['context'], question=example['question'])\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        path_answers = []\n",
        "        for _ in range(num_paths):\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(**inputs, max_new_tokens=150, do_sample=True, temperature=0.7, top_k=50, pad_token_id=tokenizer.eos_token_id)\n",
        "            completion = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(prompt):].strip()\n",
        "            match = re.search(r'A resposta final é:\\s*(.*)', completion, re.IGNORECASE)\n",
        "            if match: path_answers.append(match.group(1).strip())\n",
        "\n",
        "        final_prediction = Counter(path_answers).most_common(1)[0][0] if path_answers else \"\"\n",
        "        predictions.append({'id': example['id'], 'prediction_text': final_prediction})\n",
        "        references.append({'id': example['id'], 'answers': example['answers']})\n",
        "    return metrics_calculator.compute(predictions=predictions, references=references)\n",
        "\n",
        "def run_generic_evaluation(model, tokenizer, eval_dataset, metrics_calculator, prompt_template, strategy_name, eval_subset_size=50):\n",
        "    \"\"\"Função genérica que executa a avaliação para a maioria das estratégias de prompt.\"\"\"\n",
        "    print(f\"\\n--- Running Evaluation: {strategy_name} ---\")\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.eval()\n",
        "    predictions, references = [], []\n",
        "\n",
        "    for example in tqdm(eval_dataset.select(range(eval_subset_size)), desc=f\"Evaluating {strategy_name}\"):\n",
        "        prompt = prompt_template.format(context=example['context'], question=example['question'])\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1536, truncation=True).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs, max_new_tokens=60, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "        full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        predicted_answer = full_text[len(prompt):].strip()\n",
        "\n",
        "        if \"A resposta final é:\" in prompt:\n",
        "            match = re.search(r'A resposta final é:\\s*(.*)', predicted_answer, re.IGNORECASE)\n",
        "            predicted_answer = match.group(1).strip() if match else predicted_answer.split('\\n')[-1].strip()\n",
        "\n",
        "        predictions.append({'id': example['id'], 'prediction_text': predicted_answer})\n",
        "        references.append({'id': example['id'], 'answers': example['answers']})\n",
        "\n",
        "    return metrics_calculator.compute(predictions=predictions, references=references)"
      ],
      "metadata": {
        "id": "prompting_functions"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Execução do Pipeline e Geração de Relatório (Fase 3)"
      ],
      "metadata": {
        "id": "markdown_phase_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "def run_real_workflow(run_training=False, eval_subset_size=50):\n",
        "    \"\"\"\n",
        "    Orquestra o pipeline completo e REAL: setup, treinamento (opcional), avaliação e relatório.\n",
        "\n",
        "    Args:\n",
        "        run_training (bool): Se True, executa a demorada fase de treinamento dos modelos KD.\n",
        "                             Execute como True uma vez para gerar os modelos. Depois, use False.\n",
        "        eval_subset_size (int): Número de exemplos do dataset de validação para usar na avaliação.\n",
        "                                Aumente para uma avaliação mais robusta.\n",
        "    \"\"\"\n",
        "    # --- FASE 0: SETUP ---\n",
        "    print(\"--- FASE 0: CONFIGURANDO AMBIENTE E DADOS ---\")\n",
        "    validation_data, metrics_calc = configure_task_and_metrics()\n",
        "    prepared_data = prepare_datasets(validation_data)\n",
        "    config = KDConfig()\n",
        "\n",
        "    # --- FASE 1: TREINAMENTO (Opcional) ---\n",
        "    if run_training:\n",
        "        print(\"\\n--- FASE 1: EXECUTANDO TREINAMENTO DOS MODELOS KD ---\")\n",
        "        # Carrega os modelos base necessários para todos os treinamentos\n",
        "        student_model_base, student_tokenizer = load_model(\"google/gemma-2b\")\n",
        "        teacher_model_ext, teacher_tokenizer = load_model(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
        "\n",
        "        # 1. Treina o modelo de KD Base\n",
        "        run_base_kd_training(\n",
        "            student_model=student_model_base, teacher_model=teacher_model_ext, tokenizer=tokenizer,\n",
        "            kd_dataset=prepared_data['kd_transfer'], config=config, output_dir=\"./results/kd_base_model\"\n",
        "        )\n",
        "\n",
        "        # 2. Treina o modelo de Auto-Destilação (requer um professor \"especialista\")\n",
        "        print(\"\\n--- Treinando professor para Auto-Destilação ---\")\n",
        "        # A forma mais simples de criar um professor especialista é com um fine-tuning padrão.\n",
        "        # Aqui, vamos simular isso treinando o próprio estudante com KD por uma época.\n",
        "        specialist_teacher = run_base_kd_training(\n",
        "            student_model=student_model_base, teacher_model=teacher_model_ext, tokenizer=tokenizer,\n",
        "            kd_dataset=prepared_data['kd_transfer'], config=config, output_dir=\"./results/specialist_teacher_for_self_distillation\"\n",
        "        )\n",
        "        print(\"\\n--- Executando Auto-Destilação ---\")\n",
        "        run_base_kd_training(\n",
        "            student_model=student_model_base, teacher_model=specialist_teacher, tokenizer=tokenizer,\n",
        "            kd_dataset=prepared_data['kd_transfer'], config=config, output_dir=\"./results/kd_self_distilled_model\"\n",
        "        )\n",
        "\n",
        "        # 3. Treina o modelo de Destilação de Explicações\n",
        "        explanation_dataset = generate_explanation_dataset(teacher_model_ext, tokenizer, prepared_data['kd_transfer'])\n",
        "        run_explanation_kd_training(\n",
        "            student_model=student_model_base, tokenizer=tokenizer, explanation_dataset=explanation_dataset,\n",
        "            config=config, output_dir=\"./results/explanation_distilled_model\"\n",
        "        )\n",
        "        print(\"\\n--- FASE DE TREINAMENTO CONCLUÍDA ---\")\n",
        "\n",
        "    # --- FASE 2 & 3: AVALIAÇÃO E ANÁLISE ---\n",
        "    print(\"\\n--- FASES 2 & 3: EXECUTANDO AVALIAÇÃO COMPLETA E GERANDO RELATÓRIO ---\")\n",
        "    model_definitions = {\n",
        "        \"SLM Base\": \"google/gemma-2b\",\n",
        "        \"KD Base\": \"./results/kd_base_model\",\n",
        "        \"KD Auto-Destilado\": \"./results/kd_self_distilled_model\",\n",
        "        \"KD com Explicações\": \"./results/explanation_distilled_model\"\n",
        "    }\n",
        "    prompting_strategies = {\n",
        "        \"Zero-Shot Simples\": run_zero_shot_baseline,\n",
        "        \"ICL (k=3)\": lambda m, t, d, met: run_few_shot_icl(m, t, d, prepared_data['icl_cot_examples'], 3, met),\n",
        "        \"Zero-Shot CoT\": run_zero_shot_cot,\n",
        "        \"Auto-Consistência (n=5)\": lambda m, t, d, met: run_self_consistency_cot(m, t, d, met, 5)\n",
        "    }\n",
        "    results_df = pd.DataFrame(index=model_definitions.keys(), columns=prompting_strategies.keys())\n",
        "\n",
        "    # Carrega o modelo base uma vez para aplicar os adaptadores PEFT\n",
        "    base_model_for_eval, tokenizer = load_model(\"google/gemma-2b\")\n",
        "\n",
        "    for alias, path in model_definitions.items():\n",
        "        print(f\"\\n>>>> AVALIANDO MODELO: {alias} <<<<\")\n",
        "        if alias == \"SLM Base\":\n",
        "            model = base_model_for_eval\n",
        "        else:\n",
        "            try:\n",
        "                # Carrega o modelo base com o adaptador LoRA treinado\n",
        "                model = PeftModel.from_pretrained(base_model_for_eval, path)\n",
        "                model = model.merge_and_unload() # Opcional: mescla os pesos para acelerar a inferência\n",
        "            except Exception as e:\n",
        "                print(f\"Não foi possível carregar o modelo treinado de '{path}'. Pulando. Erro: {e}\")\n",
        "                continue\n",
        "\n",
        "        for strat_name, strat_func in prompting_strategies.items():\n",
        "            # A função de avaliação genérica agora é chamada por suas wrappers específicas\n",
        "            results = strat_func(model, tokenizer, prepared_data['evaluation'], metrics_calc)\n",
        "            results_df.loc[alias, strat_name] = f\"{results['f1']:.1f} / {results['exact_match']:.1f}\"\n",
        "\n",
        "    # --- RELATÓRIO FINAL ---\n",
        "    print(\"\\n\\n## Relatório Final de Análise ##\")\n",
        "    numeric_df = results_df.applymap(lambda x: float(x.split('/')[0]) if isinstance(x, str) else 0)\n",
        "\n",
        "    print(\"\\n--- Matriz de Resultados (F1-Score / Exact Match) ---\")\n",
        "    print(results_df.to_markdown())\n",
        "\n",
        "    print(\"\\n--- Conclusões da Análise ---\")\n",
        "    avg_kd_performance = numeric_df.drop(\"SLM Base\").mean(axis=1)\n",
        "    print(f\"- Melhor Técnica de KD (Média F1): '{avg_kd_performance.idxmax()}' ({avg_kd_performance.max():.2f})\")\n",
        "    avg_prompt_performance = numeric_df.mean(axis=0)\n",
        "    print(f\"- Melhor Estratégia de Prompting (Média F1): '{avg_prompt_performance.idxmax()}' ({avg_prompt_performance.max():.2f})\")\n",
        "    best_combo = numeric_df.stack().idxmax()\n",
        "    print(f\"- Melhor Sinergia (Modelo + Prompt): '{best_combo[0]}' + '{best_combo[1]}' com F1 de {numeric_df.max().max():.2f}\")\n",
        "\n",
        "    print(\"\\nGerando gráfico de comparação...\")\n",
        "    fig, ax = plt.subplots(figsize=(12, 7))\n",
        "    numeric_df.plot(kind='bar', ax=ax, title='Comparação de Performance (F1-Score) entre Modelos e Estratégias')\n",
        "    ax.set_ylabel(\"F1-Score\")\n",
        "    ax.set_xlabel(\"Versão do Modelo\")\n",
        "    ax.tick_params(axis='x', rotation=20)\n",
        "    plt.legend(title='Estratégia de Prompt', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"real_run_full_performance_comparison.png\")\n",
        "    print(\"Gráfico salvo em 'real_run_full_performance_comparison.png'.\")"
      ],
      "metadata": {
        "id": "uMnh8PID06Wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Execução do Pipeline (Modo Simulado)\n",
        "\n",
        "A célula abaixo executa o pipeline em **modo de simulação**. Ela não treina nem avalia os modelos de verdade, mas gera um relatório com dados de exemplo para demonstrar a estrutura de análise. Para uma execução real, você deve primeiro executar as células de treinamento da Fase 1 e depois rodar o pipeline com `simulation_mode=False`."
      ],
      "metadata": {
        "id": "markdown_run_pipeline"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: login hugging face\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "5d46296f695b44b19e3b10ed534e7d8c",
            "02e745a6dc56472baae2ef5733b1c63d",
            "2336710c68854cca889a9142efd81825",
            "97eb7ce9f5fd4f128e15724be5fe74aa",
            "c88da374e42a4ea79f0eb2715fe24a0f",
            "bbc948efb0734724ab7ccd56654b05f1",
            "2e746c17055c4d319db73aea6ffcf91d",
            "cd6bed10f72a4abe87d5f238595a1504",
            "abab12c35fc14549a0f854f343a5028e",
            "d556d17b4043458d9b6a32bf1a53fa95",
            "fea39897cab94ccab0d7ac7151873a52",
            "6f2e5ccad8bf4ce4be9d061ee67a9740",
            "7320592180ad4a188f0020db514d4ece",
            "ecbbaf48b56a4620b2a211ee4392d5a9",
            "26136d029e274b6e99bc71ff7c7f86ed",
            "0dab0e4275124a23b9b5dce2d3a53c54",
            "fd8e2425c16142e2a64ad65fb9c7846d",
            "11adbbe5a15b4a78934d8281fa40b0e1",
            "c5f57d4a83ba4303957af3a39f885aa9",
            "f237b8dcca8f4c14a8226b09c5462f8f"
          ]
        },
        "id": "Nwo8_KdnLleN",
        "outputId": "bf52cdc3-043a-4227-e463-c8b1b965a54d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5d46296f695b44b19e3b10ed534e7d8c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_full_pipeline(simulation_mode=True)"
      ],
      "metadata": {
        "id": "run-pipeline",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "798bbdcf-9b60-47b9-df11-04557516311b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'run_full_pipeline' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-51570dfb6e7b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_full_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimulation_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'run_full_pipeline' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "def run_real_workflow(run_training=False, eval_subset_size=50):\n",
        "    \"\"\"\n",
        "    Orquestra o pipeline completo e REAL: setup, treinamento (opcional), avaliação e relatório.\n",
        "\n",
        "    Args:\n",
        "        run_training (bool): Se True, executa a demorada fase de treinamento dos modelos KD.\n",
        "                             Execute como True uma vez para gerar os modelos. Depois, use False.\n",
        "        eval_subset_size (int): Número de exemplos do dataset de validação para usar na avaliação.\n",
        "                                Aumente para uma avaliação mais robusta.\n",
        "    \"\"\"\n",
        "    # --- FASE 0: SETUP ---\n",
        "    print(\"--- FASE 0: CONFIGURANDO AMBIENTE E DADOS ---\")\n",
        "    validation_data, metrics_calc = configure_task_and_metrics()\n",
        "    prepared_data = prepare_datasets(validation_data)\n",
        "    config = KDConfig()\n",
        "\n",
        "    # --- FASE 1: TREINAMENTO (Opcional) ---\n",
        "    if run_training:\n",
        "        print(\"\\n--- FASE 1: EXECUTANDO TREINAMENTO DOS MODELOS KD ---\")\n",
        "        # Carrega os modelos base necessários para todos os treinamentos\n",
        "        student_model_base, student_tokenizer = load_model(\"google/gemma-2b\")\n",
        "        teacher_model_ext, teacher_tokenizer = load_model(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
        "        print(teacher_model_ext)\n",
        "\n",
        "        # 1. Treina o modelo de KD Base\n",
        "        run_base_kd_training(\n",
        "            student_model=student_model_base, teacher_model=teacher_model_ext, student_tokenizer=student_tokenizer, teacher_tokenizer=teacher_tokenizer,\n",
        "            kd_dataset=prepared_data['kd_transfer'], config=config, output_dir=\"./results/kd_base_model\"\n",
        "        )\n",
        "\n",
        "        # 2. Treina o modelo de Auto-Destilação (requer um professor \"especialista\")\n",
        "        print(\"\\n--- Treinando professor para Auto-Destilação ---\")\n",
        "        # A forma mais simples de criar um professor especialista é com um fine-tuning padrão.\n",
        "        # Aqui, vamos simular isso treinando o próprio estudante com KD por uma época.\n",
        "        specialist_teacher = run_base_kd_training(\n",
        "            student_model=student_model_base, teacher_model=teacher_model_ext, student_tokenizer=student_tokenizer, teacher_tokenizer=teacher_tokenizer,\n",
        "            kd_dataset=prepared_data['kd_transfer'], config=config, output_dir=\"./results/specialist_teacher_for_self_distillation\"\n",
        "        )\n",
        "        print(\"\\n--- Executando Auto-Destilação ---\")\n",
        "        run_base_kd_training(\n",
        "            student_model=student_model_base, teacher_model=specialist_teacher, student_tokenizer=student_tokenizer, teacher_tokenizer=teacher_tokenizer,\n",
        "            kd_dataset=prepared_data['kd_transfer'], config=config, output_dir=\"./results/kd_self_distilled_model\"\n",
        "        )\n",
        "\n",
        "        # 3. Treina o modelo de Destilação de Explicações\n",
        "        explanation_dataset = generate_explanation_dataset(teacher_model_ext, teacher_tokenizer, prepared_data['kd_transfer'])\n",
        "        run_explanation_kd_training(\n",
        "            student_model=student_model_base, student_tokenizer=student_tokenizer, teacher_tokenizer=teacher_tokenizer, explanation_dataset=explanation_dataset,\n",
        "            config=config, output_dir=\"./results/explanation_distilled_model\"\n",
        "        )\n",
        "        print(\"\\n--- FASE DE TREINAMENTO CONCLUÍDA ---\")\n",
        "\n",
        "    # --- FASE 2 & 3: AVALIAÇÃO E ANÁLISE ---\n",
        "    print(\"\\n--- FASES 2 & 3: EXECUTANDO AVALIAÇÃO COMPLETA E GERANDO RELATÓRIO ---\")\n",
        "    model_definitions = {\n",
        "        \"SLM Base\": \"google/gemma-2b\",\n",
        "        \"KD Base\": \"./results/kd_base_model\",\n",
        "        \"KD Auto-Destilado\": \"./results/kd_self_distilled_model\",\n",
        "        \"KD com Explicações\": \"./results/explanation_distilled_model\"\n",
        "    }\n",
        "    prompting_strategies = {\n",
        "        \"Zero-Shot Simples\": run_zero_shot_baseline,\n",
        "        \"ICL (k=3)\": lambda m, t, d, met: run_few_shot_icl(m, t, d, prepared_data['icl_cot_examples'], 3, met),\n",
        "        \"Zero-Shot CoT\": run_zero_shot_cot,\n",
        "        \"Auto-Consistência (n=5)\": lambda m, t, d, met: run_self_consistency_cot(m, t, d, met, 5)\n",
        "    }\n",
        "    results_df = pd.DataFrame(index=model_definitions.keys(), columns=prompting_strategies.keys())\n",
        "\n",
        "    # Carrega o modelo base uma vez para aplicar os adaptadores PEFT\n",
        "    base_model_for_eval, tokenizer = load_model(\"google/gemma-2b\")\n",
        "\n",
        "    for alias, path in model_definitions.items():\n",
        "        print(f\"\\n>>>> AVALIANDO MODELO: {alias} <<<<\")\n",
        "        if alias == \"SLM Base\":\n",
        "            model = base_model_for_eval\n",
        "        else:\n",
        "            try:\n",
        "                # Carrega o modelo base com o adaptador LoRA treinado\n",
        "                model = PeftModel.from_pretrained(base_model_for_eval, path)\n",
        "                model = model.merge_and_unload() # Opcional: mescla os pesos para acelerar a inferência\n",
        "            except Exception as e:\n",
        "                print(f\"Não foi possível carregar o modelo treinado de '{path}'. Pulando. Erro: {e}\")\n",
        "                continue\n",
        "\n",
        "        for strat_name, strat_func in prompting_strategies.items():\n",
        "            # A função de avaliação genérica agora é chamada por suas wrappers específicas\n",
        "            results = strat_func(model, tokenizer, prepared_data['evaluation'], metrics_calc)\n",
        "            results_df.loc[alias, strat_name] = f\"{results['f1']:.1f} / {results['exact_match']:.1f}\"\n",
        "\n",
        "    # --- RELATÓRIO FINAL ---\n",
        "    print(\"\\n\\n## Relatório Final de Análise ##\")\n",
        "    numeric_df = results_df.applymap(lambda x: float(x.split('/')[0]) if isinstance(x, str) else 0)\n",
        "\n",
        "    print(\"\\n--- Matriz de Resultados (F1-Score / Exact Match) ---\")\n",
        "    print(results_df.to_markdown())\n",
        "\n",
        "    print(\"\\n--- Conclusões da Análise ---\")\n",
        "    avg_kd_performance = numeric_df.drop(\"SLM Base\").mean(axis=1)\n",
        "    print(f\"- Melhor Técnica de KD (Média F1): '{avg_kd_performance.idxmax()}' ({avg_kd_performance.max():.2f})\")\n",
        "    avg_prompt_performance = numeric_df.mean(axis=0)\n",
        "    print(f\"- Melhor Estratégia de Prompting (Média F1): '{avg_prompt_performance.idxmax()}' ({avg_prompt_performance.max():.2f})\")\n",
        "    best_combo = numeric_df.stack().idxmax()\n",
        "    print(f\"- Melhor Sinergia (Modelo + Prompt): '{best_combo[0]}' + '{best_combo[1]}' com F1 de {numeric_df.max().max():.2f}\")\n",
        "\n",
        "    print(\"\\nGerando gráfico de comparação...\")\n",
        "    fig, ax = plt.subplots(figsize=(12, 7))\n",
        "    numeric_df.plot(kind='bar', ax=ax, title='Comparação de Performance (F1-Score) entre Modelos e Estratégias')\n",
        "    ax.set_ylabel(\"F1-Score\")\n",
        "    ax.set_xlabel(\"Versão do Modelo\")\n",
        "    ax.tick_params(axis='x', rotation=20)\n",
        "    plt.legend(title='Estratégia de Prompt', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"real_run_full_performance_comparison.png\")\n",
        "    print(\"Gráfico salvo em 'real_run_full_performance_comparison.png'.\")"
      ],
      "metadata": {
        "id": "7qySGyuBLwrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_real_workflow(True, 10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426,
          "referenced_widgets": [
            "98b304a4e8bd45488839877687d4c346",
            "4a9a3b739a9746268b8935f17bea382c",
            "85a4be3f0db44665b23c7c7b5325ea03",
            "45ef3133611e4da4ba69c6a80a0b9860",
            "52db1df8091d46928516be4eebc7f5ca",
            "dc793fffa9a74823b4c250a897e6c80e",
            "ffc3adac99a8435dba04c494e41118e9",
            "3bf241642ed04c999d95c681e5cf1df5",
            "d01196581b804d2781ca3c88a955228e",
            "2b1c3c2352a44df7a6da5f96f015d6c0",
            "519d719e80b64aa7a7021aa609dfe8e8",
            "c2a38335034b49e093213144bcfa0d31",
            "5f9a19b6335c40e78a095b4fe7f57667",
            "e75bb0f9a87445d3a025a0b2f4a2ddbc",
            "53459e5db4214daa86d6e97113088e82",
            "f127d4bc0b8e4b3a97569ebdc7c9952b",
            "f6671d0a74a446f4bbb7d8bb4263e8de",
            "46443d1167554710b82fdbd299fa65b0",
            "ef69db249a4344c3946903d5ae2a4ce6",
            "4c35eab1b8cc49b38e0258a2cbf9a487",
            "53817bec58e04feb88aeaead1249c37e",
            "6788985ac10f4087b814ad71ee9f713d"
          ]
        },
        "id": "e2zr_LJsNAIM",
        "outputId": "5af55386-bdba-4586-c911-d3d2e359c661"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- FASE 0: CONFIGURANDO AMBIENTE E DADOS ---\n",
            "--- Step 0.1: Configuring Task (SQuAD) and Metrics ---\n",
            "Dataset e métricas carregados.\n",
            "\n",
            "--- Step 0.4: Preparing Datasets ---\n",
            "Datasets preparados.\n",
            "\n",
            "--- FASE 1: EXECUTANDO TREINAMENTO DOS MODELOS KD ---\n",
            "\n",
            "--- Loading Model: google/gemma-2b ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "98b304a4e8bd45488839877687d4c346"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo carregado com sucesso.\n",
            "\n",
            "--- Loading Model: mistralai/Mistral-7B-Instruct-v0.2 ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2a38335034b49e093213144bcfa0d31"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Erro ao carregar o modelo: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "ERRO CRÍTICO: Falha ao carregar um dos modelos base. O pipeline não pode continuar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FASE 0: SETUP ---\n",
        "print(\"--- FASE 0: CONFIGURANDO AMBIENTE E DADOS ---\")\n",
        "validation_data, metrics_calc = configure_task_and_metrics()\n",
        "prepared_data = prepare_datasets(validation_data)\n",
        "config = KDConfig()\n",
        "\n",
        "run_training  = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-exjfMDD_8F",
        "outputId": "cbf4f494-a5fa-4928-fb76-a981d5bbee44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- FASE 0: CONFIGURANDO AMBIENTE E DADOS ---\n",
            "--- Step 0.1: Configuring Task (SQuAD) and Metrics ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset e métricas carregados.\n",
            "\n",
            "--- Step 0.4: Preparing Datasets ---\n",
            "Datasets preparados.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FASE 1: TREINAMENTO (Opcional) ---\n",
        "\n",
        "print(\"\\n--- FASE 1: EXECUTANDO TREINAMENTO DOS MODELOS KD ---\")\n",
        "# Carrega os modelos base necessários para todos os treinamentos\n",
        "student_model_base, student_tokenizer = load_model(\"google/gemma-2b\")\n",
        "teacher_model_ext, teacher_tokenizer = load_model(\"mistralai/Mistral-7B-Instruct-v0.2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244,
          "referenced_widgets": [
            "f7313eb452b945abaac6b64ead90e703",
            "645a6963efa74f999df3421091f6b5b7",
            "ccd4201b231741c3aa4dd5a18ff961b1",
            "11b58d107b4d4a5fb44a5e01046e1bd9",
            "51b0b23c7ad449c5a4631e8d2c592ef4",
            "be6b6fb0f4c9433198bbf7cbca5c7874",
            "fe2d2f2b05324dfbbec4e813f8aeb2ab",
            "ebcd3a71ad9f444da9fe71e3db1adabe",
            "399ddfc042e9492a85bbdf7cc348cc23",
            "4eebd8df447540ecbcc9287ee34fbc94",
            "1c524b94325342e893e48ab4da3139e9",
            "1e6ba4e3ef3a446aa9bfe53291ac1562",
            "6dc30bdfb5b54ed0b8743d2362181256",
            "ec0e17f63be444fbab699dfa3e723bf3",
            "7ac611c0386d47f0b2c5231336da029e",
            "40c386b42d4d4fb18d297b4dc1df1bd5",
            "442f704806e448dcbfb1d5271f1fa2ab",
            "60c2c4506ccd4995b813ae2980f04e28",
            "b2d91432b63042a3a6191ce686f1edcd",
            "e8fee87eef93433a9124f48ad040666e",
            "2ecf1a0bc6af42bfb6969936fc757a23",
            "20bf94add42f468e93015845e4fd2a72"
          ]
        },
        "id": "hoj0SmeaELER",
        "outputId": "2b42eada-fdd7-4304-8b78-7266afc37c67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- FASE 1: EXECUTANDO TREINAMENTO DOS MODELOS KD ---\n",
            "\n",
            "--- Loading Model: google/gemma-2b ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f7313eb452b945abaac6b64ead90e703"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo carregado com sucesso.\n",
            "\n",
            "--- Loading Model: mistralai/Mistral-7B-Instruct-v0.2 ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e6ba4e3ef3a446aa9bfe53291ac1562"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo carregado com sucesso.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Treina o modelo de KD Base\n",
        "run_base_kd_training(\n",
        "    student_model=student_model_base, teacher_model=teacher_model_ext, student_tokenizer=student_tokenizer, teacher_tokenizer=teacher_tokenizer,\n",
        "    kd_dataset=prepared_data['kd_transfer'], config=config, output_dir=\"./results/kd_base_model\"\n",
        ")\n",
        "\n",
        "# 2. Treina o modelo de Auto-Destilação (requer um professor \"especialista\")\n",
        "print(\"\\n--- Treinando professor para Auto-Destilação ---\")\n",
        "# A forma mais simples de criar um professor especialista é com um fine-tuning padrão.\n",
        "# Aqui, vamos simular isso treinando o próprio estudante com KD por uma época.\n",
        "specialist_teacher = run_base_kd_training(\n",
        "    student_model=student_model_base, teacher_model=teacher_model_ext, student_tokenizer=student_tokenizer, teacher_tokenizer=teacher_tokenizer,\n",
        "    kd_dataset=prepared_data['kd_transfer'], config=config, output_dir=\"./results/specialist_teacher_for_self_distillation\"\n",
        ")\n",
        "print(\"\\n--- Executando Auto-Destilação ---\")\n",
        "run_base_kd_training(\n",
        "    student_model=student_model_base, teacher_model=specialist_teacher, student_tokenizer=student_tokenizer, teacher_tokenizer=teacher_tokenizer,\n",
        "    kd_dataset=prepared_data['kd_transfer'], config=config, output_dir=\"./results/kd_self_distilled_model\"\n",
        ")\n",
        "\n",
        "# 3. Treina o modelo de Destilação de Explicações\n",
        "explanation_dataset = generate_explanation_dataset(teacher_model_ext, teacher_tokenizer, prepared_data['kd_transfer'])\n",
        "run_explanation_kd_training(\n",
        "    student_model=student_model_base, student_tokenizer=student_tokenizer, teacher_tokenizer=teacher_tokenizer, explanation_dataset=explanation_dataset,\n",
        "    config=config, output_dir=\"./results/explanation_distilled_model\"\n",
        ")\n",
        "print(\"\\n--- FASE DE TREINAMENTO CONCLUÍDA ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529,
          "referenced_widgets": [
            "ab477ce71f0d4569b6efe5e6e2b6fd03",
            "fba007a9e58a422aabe9cdc2e911fb11",
            "f53f017bf5924d3698754356c29fd7a1",
            "28a01690abed47d1aff9f3c410679ce4",
            "bfd008ed77ea43cdbc9a2aee0c84da0e",
            "1f69c06071ec44ad89795297e416a3dd",
            "6477b0805ac14c4eb7a21380cd02da9d",
            "e553017af8ef42a3b3335ab74164ab5b",
            "4dff25c9975441d395b9d4b9e582b08b",
            "024ed6944a2f4263b2903ecdf03f05c4",
            "8b7d193a6b844813b84ab3404fc95677"
          ]
        },
        "id": "I9mMGD2IE1Xw",
        "outputId": "bee6b90c-a45e-47ac-f4fa-87d0bdc951d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Running Base Knowledge Distillation -> saving to ./results/kd_base_model ---\n",
            "\n",
            "Epoch 1/1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 1:   0%|          | 0/200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab477ce71f0d4569b6efe5e6e2b6fd03"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 367409 has 14.73 GiB memory in use. Of the allocated memory 14.58 GiB is allocated by PyTorch, and 30.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-8ce78d310d2a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 1. Treina o modelo de KD Base\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m run_base_kd_training(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mstudent_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstudent_model_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mteacher_model_ext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudent_tokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstudent_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_tokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mteacher_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mkd_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'kd_transfer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./results/kd_base_model\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;32m<ipython-input-6-ff7514fb43f6>\u001b[0m in \u001b[0;36mrun_base_kd_training\u001b[0;34m(student_model, teacher_model, student_tokenizer, teacher_tokenizer, kd_dataset, config, output_dir)\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mteacher_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mteacher_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mteacher_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mstudent_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpeft_student_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mstudent_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# A perda é calculada entre os logits, que devem ter o mesmo tamanho de vocabulário\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1755\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1756\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m                 return self.base_model(\n\u001b[0m\u001b[1;32m   1758\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1759\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma/modeling_gemma.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    684\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma/modeling_gemma.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    450\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_checkpointing\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma/modeling_gemma.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma/modeling_gemma.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdown_proj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnew_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mpre_forward\u001b[0;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtied_pointers_to_remove\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m                 set_module_tensor_to_device(\n\u001b[0m\u001b[1;32m    361\u001b[0m                     \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                     \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/modeling.py\u001b[0m in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    335\u001b[0m                     \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mold_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0mnew_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0mnew_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 367409 has 14.73 GiB memory in use. Of the allocated memory 14.58 GiB is allocated by PyTorch, and 30.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FASE 2 & 3: AVALIAÇÃO E ANÁLISE ---\n",
        "print(\"\\n--- FASES 2 & 3: EXECUTANDO AVALIAÇÃO COMPLETA E GERANDO RELATÓRIO ---\")\n",
        "model_definitions = {\n",
        "    \"SLM Base\": \"google/gemma-2b\",\n",
        "    \"KD Base\": \"./results/kd_base_model\",\n",
        "    \"KD Auto-Destilado\": \"./results/kd_self_distilled_model\",\n",
        "    \"KD com Explicações\": \"./results/explanation_distilled_model\"\n",
        "}\n",
        "prompting_strategies = {\n",
        "    \"Zero-Shot Simples\": run_zero_shot_baseline,\n",
        "    \"ICL (k=3)\": lambda m, t, d, met: run_few_shot_icl(m, t, d, prepared_data['icl_cot_examples'], 3, met),\n",
        "    \"Zero-Shot CoT\": run_zero_shot_cot,\n",
        "    \"Auto-Consistência (n=5)\": lambda m, t, d, met: run_self_consistency_cot(m, t, d, met, 5)\n",
        "}\n",
        "results_df = pd.DataFrame(index=model_definitions.keys(), columns=prompting_strategies.keys())"
      ],
      "metadata": {
        "id": "OrnYoUT6EOcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega o modelo base uma vez para aplicar os adaptadores PEFT\n",
        "base_model_for_eval, tokenizer = load_model(\"google/gemma-2b\")\n",
        "\n",
        "for alias, path in model_definitions.items():\n",
        "    print(f\"\\n>>>> AVALIANDO MODELO: {alias} <<<<\")\n",
        "    if alias == \"SLM Base\":\n",
        "        model = base_model_for_eval\n",
        "    else:\n",
        "        try:\n",
        "            # Carrega o modelo base com o adaptador LoRA treinado\n",
        "            model = PeftModel.from_pretrained(base_model_for_eval, path)\n",
        "            model = model.merge_and_unload() # Opcional: mescla os pesos para acelerar a inferência\n",
        "        except Exception as e:\n",
        "            print(f\"Não foi possível carregar o modelo treinado de '{path}'. Pulando. Erro: {e}\")\n",
        "            continue\n",
        "\n",
        "    for strat_name, strat_func in prompting_strategies.items():\n",
        "        # A função de avaliação genérica agora é chamada por suas wrappers específicas\n",
        "        results = strat_func(model, tokenizer, prepared_data['evaluation'], metrics_calc)\n",
        "        results_df.loc[alias, strat_name] = f\"{results['f1']:.1f} / {results['exact_match']:.1f}\"\n",
        "\n",
        "# --- RELATÓRIO FINAL ---\n",
        "print(\"\\n\\n## Relatório Final de Análise ##\")\n",
        "numeric_df = results_df.applymap(lambda x: float(x.split('/')[0]) if isinstance(x, str) else 0)\n",
        "\n",
        "print(\"\\n--- Matriz de Resultados (F1-Score / Exact Match) ---\")\n",
        "print(results_df.to_markdown())\n",
        "\n",
        "print(\"\\n--- Conclusões da Análise ---\")\n",
        "avg_kd_performance = numeric_df.drop(\"SLM Base\").mean(axis=1)\n",
        "print(f\"- Melhor Técnica de KD (Média F1): '{avg_kd_performance.idxmax()}' ({avg_kd_performance.max():.2f})\")\n",
        "avg_prompt_performance = numeric_df.mean(axis=0)\n",
        "print(f\"- Melhor Estratégia de Prompting (Média F1): '{avg_prompt_performance.idxmax()}' ({avg_prompt_performance.max():.2f})\")\n",
        "best_combo = numeric_df.stack().idxmax()\n",
        "print(f\"- Melhor Sinergia (Modelo + Prompt): '{best_combo[0]}' + '{best_combo[1]}' com F1 de {numeric_df.max().max():.2f}\")\n",
        "\n",
        "print(\"\\nGerando gráfico de comparação...\")\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "numeric_df.plot(kind='bar', ax=ax, title='Comparação de Performance (F1-Score) entre Modelos e Estratégias')\n",
        "ax.set_ylabel(\"F1-Score\")\n",
        "ax.set_xlabel(\"Versão do Modelo\")\n",
        "ax.tick_params(axis='x', rotation=20)\n",
        "plt.legend(title='Estratégia de Prompt', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"real_run_full_performance_comparison.png\")\n",
        "print(\"Gráfico salvo em 'real_run_full_performance_comparison.png'.\")"
      ],
      "metadata": {
        "id": "JXVjH7h1Ede_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}